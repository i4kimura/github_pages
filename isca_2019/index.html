



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.0.2">
    
    
      
        <title>Isca 2019 - FPGA開発日記 カテゴリ別記事インデックス</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.982221ab.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../assets/javascripts/modernizr.1f0bcf2b.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#session-1a-using-ai-to-improve-architecture" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="FPGA開発日記 カテゴリ別記事インデックス" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              FPGA開発日記 カテゴリ別記事インデックス
            </span>
            <span class="md-header-nav__topic">
              Isca 2019
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href=".." title="FPGA開発日記 カテゴリ別記事インデックス" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    FPGA開発日記 カテゴリ別記事インデックス
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../30os/" title="オペレーティングシステム" class="md-nav__link">
      オペレーティングシステム
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../cpu/" title="CPU" class="md-nav__link">
      CPU
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../dsl_ruby/" title="DSLでビルドツールを自作する" class="md-nav__link">
      DSLでビルドツールを自作する
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../gpgpu/" title="GPGPU 記事一覧" class="md-nav__link">
      GPGPU 記事一覧
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Isca 2019
      </label>
    
    <a href="./" title="Isca 2019" class="md-nav__link md-nav__link--active">
      Isca 2019
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#session-1a-using-ai-to-improve-architecture" title="Session 1A. Using AI to Improve Architecture" class="md-nav__link">
    Session 1A. Using AI to Improve Architecture
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#session-1b-non-traditional-workload-domains" title="Session 1B: Non-Traditional Workload Domains" class="md-nav__link">
    Session 1B: Non-Traditional Workload Domains
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-2a-memory-and-storage" title="Session 2A: Memory and Storage" class="md-nav__link">
    Session 2A: Memory and Storage
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-2b-gpus" title="Session 2B: GPUs" class="md-nav__link">
    Session 2B: GPUs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-3a-deep-learning-and-ai" title="Session 3A: Deep Learning and AI" class="md-nav__link">
    Session 3A: Deep Learning and AI
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-3b-security" title="Session 3B: Security" class="md-nav__link">
    Session 3B: Security
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-4a-caching-and-data-management" title="Session 4A: Caching and Data Management" class="md-nav__link">
    Session 4A: Caching and Data Management
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-4b-datacenter-and-end-to-end-compute" title="Session 4B: Datacenter and End-to-End Compute" class="md-nav__link">
    Session 4B: Datacenter and End-to-End Compute
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-5a-quantum" title="Session 5A: Quantum" class="md-nav__link">
    Session 5A: Quantum
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-5b-communication" title="Session 5B: Communication" class="md-nav__link">
    Session 5B: Communication
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-6a-software-hardware-and-prediction" title="Session 6A: Software-Hardware and Prediction" class="md-nav__link">
    Session 6A: Software-Hardware and Prediction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-6b-acceleration-and-non-traditional-architecture" title="Session 6B: Acceleration and Non-Traditional Architecture" class="md-nav__link">
    Session 6B: Acceleration and Non-Traditional Architecture
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../llvm/" title="LLVM" class="md-nav__link">
      LLVM
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../lowrisc_llvm/" title="RISC-V LLVMバックエンドのステップバイステップガイド" class="md-nav__link">
      RISC-V LLVMバックエンドのステップバイステップガイド
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../machine_learning/" title="機械学習 / TensorFlow" class="md-nav__link">
      機械学習 / TensorFlow
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../mastering_bitcoin/" title="Bitcoin 記事一覧" class="md-nav__link">
      Bitcoin 記事一覧
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../quantum_computing/" title="量子コンピュータ / Q# 記事一覧" class="md-nav__link">
      量子コンピュータ / Q# 記事一覧
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../riscv/" title="RISC-V" class="md-nav__link">
      RISC-V
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../rust/" title="Rust" class="md-nav__link">
      Rust
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../vivado_hls/" title="Vivado-HLS" class="md-nav__link">
      Vivado-HLS
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-15" type="checkbox" id="nav-15">
    
    <label class="md-nav__link" for="nav-15">
      Cq
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-15">
        Cq
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../cq/hifive_unleashed/" title="【供養】 CQ出版インターフェース2019年オープンソースRISC-V特集時にボツとなったHiFive Unleashed特集原稿" class="md-nav__link">
      【供養】 CQ出版インターフェース2019年オープンソースRISC-V特集時にボツとなったHiFive Unleashed特集原稿
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#session-1a-using-ai-to-improve-architecture" title="Session 1A. Using AI to Improve Architecture" class="md-nav__link">
    Session 1A. Using AI to Improve Architecture
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#session-1b-non-traditional-workload-domains" title="Session 1B: Non-Traditional Workload Domains" class="md-nav__link">
    Session 1B: Non-Traditional Workload Domains
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-2a-memory-and-storage" title="Session 2A: Memory and Storage" class="md-nav__link">
    Session 2A: Memory and Storage
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-2b-gpus" title="Session 2B: GPUs" class="md-nav__link">
    Session 2B: GPUs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-3a-deep-learning-and-ai" title="Session 3A: Deep Learning and AI" class="md-nav__link">
    Session 3A: Deep Learning and AI
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-3b-security" title="Session 3B: Security" class="md-nav__link">
    Session 3B: Security
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-4a-caching-and-data-management" title="Session 4A: Caching and Data Management" class="md-nav__link">
    Session 4A: Caching and Data Management
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-4b-datacenter-and-end-to-end-compute" title="Session 4B: Datacenter and End-to-End Compute" class="md-nav__link">
    Session 4B: Datacenter and End-to-End Compute
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-5a-quantum" title="Session 5A: Quantum" class="md-nav__link">
    Session 5A: Quantum
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-5b-communication" title="Session 5B: Communication" class="md-nav__link">
    Session 5B: Communication
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-6a-software-hardware-and-prediction" title="Session 6A: Software-Hardware and Prediction" class="md-nav__link">
    Session 6A: Software-Hardware and Prediction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#session-6b-acceleration-and-non-traditional-architecture" title="Session 6B: Acceleration and Non-Traditional Architecture" class="md-nav__link">
    Session 6B: Acceleration and Non-Traditional Architecture
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>Isca 2019</h1>
                
                <h2 id="session-1a-using-ai-to-improve-architecture">Session 1A. Using AI to Improve Architecture</h2>
<ul>
<li>
<p>Perceptron-based prefetch filtering</p>
</li>
<li>
<p>ハードウェアプリフェッチは、最新のプロセッサ設計においてキャッシュミスレイテンシを隠すための効果的な技術です。それは、カバレッジ、すなわちプリフェッチャがキャッシュに持ち込むベースラインキャッシュミスの割合、および精度、すなわち最終的に使用されるプリフェッチの割合です。過度に攻撃的なプリフェッチャは、精度を低下させる代償としてカバレッジを向上させるかもしれません。そのため、キャッシュ容量や帯域幅を含む多くのリソースが無駄になるため、このような過剰な攻撃性によって性能が損なわれる可能性があります。理想的なプリフェッチャは、高いカバレッジと精度の両方を備えています。</p>
<p>この論文では、精度に悪影響を与えることなく、基礎となるプリフェッチャによって生成されるプリフェッチのカバレッジを向上させる方法として、パーセプトロンベースのプリフェッチフィルタリング（PPF）を紹介します。PPFは基本的なプリフェッチャのチューニングをより積極的に行うことができ、そのような積極的なチューニングが意味する不正確なプリフェッチの数の増加をフィルタリングすることでカバレッジを向上させることができます。また、不正確なプリフェッチを識別するためにPPFのパーセプトロン層を訓練するために使用するさまざまな特徴を探求しています。PPFは、SPEC CPU 2017ベンチマークのメモリ負荷の高いサブセットにおいて、基礎となるプリフェッチャのみを使用した場合と比較して、シングルコア構成で3.78%、4コア構成で11.4%の性能向上を実現しました。</p>
</li>
<li>
<p>1A2 : Post-Silicon CPU Adaptation Made Practical Using Machine Learning  </p>
</li>
<li>
<p>実行時にワークロードにアーキテクチャを適応させるプロセッサは、ワットあたりのパフォーマンス（PPW）を向上させ、パイプラインのスケーリングによるリターンの減少を緩和するための1つの方法を提供しています。最先端の適応型CPUは、機械学習（ML）モデルをチップ上に配置し、イベントカウンタデータのワークロードパターンを認識してハードウェアを最適化します。しかし、画期的なPPWの向上にもかかわらず、このような設計は、現場での系統的な適応エラーの可能性があるため、まだ広く採用されていません。</p>
<p>本稿では、Intel SkyLakeをベースにした適応型CPUを紹介します。このCPUは、クラスタ化されたアーキテクチャのイシュー幅を動的に設定しながら、使用されていないリソースをクロックゲーティングすることで、予測的なクラスタゲーティングを行います。ゲーティングの決定は、既存のマイクロコントローラ上で実行されるML適応モデルによって駆動され、設計の複雑さを最小限に抑え、ファームウェアの更新を容易にして性能特性を調整することができます。特に、適応モデルは、新しいワークロードでパフォーマンスを低下させる危険性のある統計的なブラインドスポットに悩まされることがありますが、慎重な設計とトレーニングにより、これらの影響を最小限に抑えることができることを示しています。</p>
<p>我々の適応型CPUは、SPEC2017上の同等の非適応型CPUと比較してPPWを31.4%改善し、サービスレベルアグリーメント（SLA）違反が最先端のものと比較して2桁少ないことを示した。異なるSLAや特定のアプリケーション向けに訓練されたモデルを使用してPPWを最適化する方法を示します。結果として得られたCPUは、初めて実世界の展開基準を満たし、顧客のニーズが変化しても、個々の顧客に合わせてハードウェアをカスタマイズするための新しい手段を提供します。</p>
</li>
<li>
<p>Bit-level perceptron prediction for indirect branches</p>
</li>
<li>
<p>最近のソフトウェアは、仮想メソッドのディスパッチやスイッチ文の実装など、さまざまな目的で間接分岐を使用していますが、これに限定されません。間接分岐のターゲットアドレスは、実行前に決定することができないため、高性能プロセッサは、制御の危険性を軽減するために、精度の高い間接分岐予測技術に依存しています。</p>
<p>本論文では、ビットレベルでターゲットアドレスを予測する新しい間接分岐予測方式を提案する。一連のパーセプトロンベースの予測器を用いて、我々の予測器はブランチ履歴内の相関関係に基づいて個々のブランチターゲットアドレスのビットを予測する。我々の評価では、この新しいブランチターゲット予測器は、同等のハードウェア予算で最先端のブランチターゲット予測器と競合しないことが示されました。例えば、SPECやモバイルアプリケーションを含む一連のワークロードにおいて、我々の予測器は1000命令あたり0.183の誤予測率を達成しましたが、最先端のITTAGE予測器では0.193、VPCベースの間接予測器では0.29でした。</p>
</li>
<li>
<p>Generative and multi-phase learning for computer systems optimization</p>
</li>
<li>
<p>機械学習と人工知能はコンピュータシステムの最適化に非常に有用である：コンピュータシステムが管理のためのより多くの資源を公開しているので、ML/AIはこれらの資源の複雑な相互作用をモデル化するために必要とされています。ML/AIをコンピュータシステムに組み込むための標準的な方法は、まず学習者を訓練して資源の使用量に応じてシステムの動作を正確に予測し、学習したモデルをスケジューラなどのシステムの一部として展開することである。本論文では、(1)学習精度を向上させ続けてもシステムの結果は向上しないが、(2)システム問題の知識を学習プロセスに組み込むことで、全体の精度は向上しなくてもシステムの結果は向上することを示す。具体的には、最小限のエネルギーで遅延制約を満たすことを目標に、アプリケーションの性能と消費電力を資源使用量の関数として学習する。少ないデータを用いて学習精度を向上させる新しい生成モデルを提案し、システム問題の知識を組み込んだ多相サンプリング手法を提案する。本研究で得られた結果は、ポジティブなものとネガティブなものの両方である。生成モデルは、最先端の学習システムであっても精度を向上させるが、エネルギーにはマイナスの影響を与える。多相サンプリングは、最先端の学習システムと比較してエネルギー消費量を削減するが、精度は向上しない。これらの結果は、システム最適化のための学習は、精度の向上がシステムの結果にほとんど影響を与えない、リターンの逓減点に達している可能性があることを示唆しています。したがって、システムの学習に関する今後の研究では、精度を重視せず、代わりにシステム問題の構造を学習者に組み込むべきであることを提唱します。</p>
</li>
</ul>
<h3 id="session-1b-non-traditional-workload-domains">Session 1B: Non-Traditional Workload Domains</h3>
<ul>
<li>
<p>OO-VR: NUMA friendly <u>o</u>bject-<u>o</u>riented <u>VR</u> rendering framework for future NUMA-based multi-GPU systems</p>
</li>
<li>
<p>NUMA ベースのマルチ GPU システムは強力な計算能力を持ち、VR アプリケーションに持続的でスケーラブルな性能を提供し、優れたユーザー体験を提供するための有望な候補となっています。しかし、マルチGPUシステム全体を単一のGPUとみなす単一プログラミングモデルでは、ワークロード分散時のVRレンダリングタスク間のデータロカリティが大きく無視され、GPUモデル(GPM)間で膨大なリモートメモリアクセスが発生してしまいます。マルチ GPU システムで VR アプリケーションを実行する際には、限られた GPU 間リンク帯域幅（NVlink の場合は 64GB/s など）が大きな障害となります。様々な種類の並列レンダリングフレームワークの特性を総合的に評価した結果、GPMごとにレンダリングオブジェクトと必要なデータを分散させることで、GPM間のメモリアクセスを削減できることがわかりました。しかし、このようなオブジェクトレベルのレンダリングは、NUMAベースのマルチGPUシステムでは、以下の2つの大きな課題に直面しています。(1) 同一オブジェクトの左右のビュー間のデータロカリティが大きく、異なるオブジェクト間でデータを共有すること、(2) ソフトウェアレベルの分配・合成メカニズムによって引き起こされるワークロードのアンバランス、である。</p>
<p>これらの課題を解決するために、NUMAベースのマルチGPUシステムにおけるVRマルチビューレンダリングのためのNUMAフレンドリーなソリューションを提供するために、ソフトウェアとハードウェアの最適化を行うオブジェクト指向VRレンダリングフレームワーク(OO-VR)を提案します。まず、同じオブジェクトの2つのビュー間のデータ共有を利用し、テクスチャ共有レベルに基づいてオブジェクトをバッチにグループ化するオブジェクト指向VRプログラミングモデルを提案します。次に、オブジェクトを意識したランタイムバッチ配信エンジンと分散型ハードウェア構成ユニットを設計し、GPM間のワークロードのバランスを実現し、VRレンダリングのパフォーマンスをさらに向上させます。最後に、VRシミュレータでの評価では、最新のマルチGPUシステムと比較して、OO-VRは1.58倍の性能向上と76%のGPM間メモリトラフィック削減を実現しています。さらに、OO-VRは、ローカルメモリとリモートメモリ間の非対称帯域幅の増加に伴う将来の大規模なマルチGPUシナリオに対応したNUMAフレンドリーなパフォーマンススケーラビリティを提供します。</p>
</li>
<li>
<p>PES: proactive event scheduling for responsive and energy-efficient mobile web computing</p>
</li>
<li>
<p>Webアプリケーションは、リソースに制約のあるモバイルデバイスへと徐々に移行しています。その結果、Webランタイムシステムは、応答性とエネルギー効率という2つの課題に同時に対処しなければなりません。従来のWebランタイムシステムは、ユーザーイベントがトリガーされた後にのみ反応するというリアクティブな性質を持っているため、うまく機能しませんでした。リアクティブな戦略は、イベントの実行を一度に一つずつスケジュールするローカルな最適化につながり、グローバルな最適化の機会を逃してしまいます。</p>
<p>この論文では、プロアクティブイベントスケジューリング(PES)を提案します。PESの主な考え方は、将来のイベントを事前に予測し、それによってイベント間のスケジューリング決定をグローバルに調整することです。具体的には、統計的推論とアプリケーションコード解析を組み合わせて、近い将来に起こりそうなイベントを予測します。そしてPESは、将来のイベントを予測し、すべてのイベントのQoS制約を満たしつつ、グローバルなエネルギー消費を最小限に抑える方法で、将来のイベントを事前に実行します。基本的に、PESはスケジューリングウィンドウを拡大することで、より多くの最適化の機会を引き出し、未解決のイベントと予測されるイベントの両方にまたがる調整を可能にします。ハードウェア測定によると、PESはAndroidのデフォルトのインタラクティブCPUガバナと比較して、QoS違反を61.2%削減し、エネルギー消費を26.5%削減することがわかりました。また、最先端のリアクティブスケジューラであるEBSと比較して、QoS違反と消費電力をそれぞれ63.1%と17.9%削減しています。</p>
</li>
<li>
<p>3D-based video recognition acceleration by leveraging temporal locality</p>
</li>
<li>
<p>近年、畳み込みニューラルネットワーク(CNN)のためのドメイン固有のアクセラレータが爆発的に増えています。これまでのCNNアクセラレータの多くは画像認識のニューラルネットワークを対象としており，AlexNet, VGG, GoogleNet, ResNetなどがある．本論文では、これまでとは異なるルートで、2次元CNNよりも計算量が多く、より多くの可能性を秘めた3次元CNNの高速化を研究しています。代表的な3次元CNNの特性評価を行った後，我々は各層のイマップの時間的デルタを利用した時間次元間の差分畳み込みを利用し，時間的デルタの有効ビットのみを利用して計算をビット単位で処理している．空間ロカリティと時間ロカリティをさらに活用し，すべてのCNNに一般的なアーキテクチャとするために，空間デルタデータフローと時間デルタデータフローを動的に切り替える制御機構を提案する．我々の設計をTSVA(temporal-spatial value aware accelerator)と呼ぶ。表現NNネットワークを用いた評価の結果，TSVAは平均4.24倍の高速化と1.42倍のエネルギー効率を達成できることが示された．我々はビデオ認識のための3D CNNをターゲットとしているが，TSVAは連続的なバッチ処理のための他の一般的なCNNにも恩恵を与えることができる．</p>
</li>
<li>
<p>Energy-efficient video processing for virtual reality</p>
</li>
<li>
<p>バーチャルリアリティ(VR)は、球状パノラマ映像処理がバックボーン技術の一つであり、根本的に新しい応用が可能になる可能性を秘めています。しかし、現在のVRシステムでは、従来の平面映像処理用に設計された技術を再利用しているため、エネルギー効率が大幅に低下しています。今回の解析では、360°VRコンテンツの処理に特有の操作が処理エネルギーの40%を占めていることを明らかにしました。</p>
<p>我々は、エネルギー効率の高いVR映像処理のためのエンドツーエンドシステムであるEVRを紹介します。EVRは、VR税の主な原因が射影変換（PT）操作であることを認識しています。EVRは、サーバ上のセマンティック・アウェア・ストリーミング（SAS）とクライアント・デバイス上のハードウェア・アクセラレイテッド・レンダリング（HAR）という2つの主要な技術を用いてPTのオーバーヘッドを軽減しています。EVRはSASを使用して、クラウド上で360°フレームをプリレンダリングすることで、VRデバイス上で射影変換を実行する機会を減らします。従来のプリレンダリング技術とは異なり、SASはこれまで無視されていたVRコンテンツに内在する重要な意味情報を利用しています。SASを補完するHARは、射影変換に特化した新しいハードウェアアクセラレータを用いて、デバイス上でのレンダリングのエネルギーオーバーヘッドを軽減します。我々は、Amazon AWS サーバーインスタンスと NVIDA Jetson TX2 ボードとザイリンクス Zynq-7000 FPGA を組み合わせた EVR プロトタイプを実装しました。実際のシステム測定によると、EVRはVRレンダリングのエネルギーを最大58%削減し、VRデバイスのエネルギーを最大42%削減することがわかりました。</p>
</li>
</ul>
<h3 id="session-2a-memory-and-storage">Session 2A: Memory and Storage</h3>
<ul>
<li>
<p>Triad-NVM: persistency for integrity-protected and encrypted non-volatile memories</p>
</li>
<li>
<p>不揮発性メモリが登場し、メインメモリに魅力的なファブリックを提供します。DRAMとは異なり、不揮発性メインメモリ(NVMM)は電源喪失後もデータを保持します。これにより、メモリはクラッシュや再起動時にもデータを永続的に保持することができますが、攻撃者が起動時のエピソードの間にデータを盗み見したり改ざんしたりする可能性が出てきます。メモリの暗号化と完全性の検証は DRAM システムではよく研究されてきましたが、セキュリティ保証、クラッシュ/リブート時のデータリカバリ、優れた永続性パフォーマンス、高速リカバリを同時に実現するには、NVMM では新たな課題が浮上しています。</p>
<p>この論文では、安全な永続性を実現するために、すべてのセキュリティメタデータ（カウンタ、MAC、メルクルツリー）を使用したデータの永続性を探求します。セキュリティ保証を確保するためには、カウンタに加えて、メッセージ認証コード（MAC）と盆栽メルクルツリー（BMT）を維持する必要があり、これらが永続性オーバーヘッドの大部分を提供することを示す。我々は、永続的なメモリ領域と非永続的なメモリ領域の両方について、安全な永続性を実現するための要件を分析しています。メモリの非揮発性の性質は、再起動時に整合性検証の失敗を引き起こす可能性があることを発見したので、非永続メモリ領域をサポートするための別のメカニズムを提案する。第四に、リカバリを高速化するための設計を提案する。提案したTriad-NVMは、厳密な永続化と比較して平均2倍のスループット向上が可能であることを示した。さらに、Triad-NVMは、セキュリティメタデータの永続性を持たないシステムと比較して、リカバリ時間を桁違いに高速化することができます。</p>
</li>
<li>
<p>GraphSSD: graph semantics aware SSD</p>
</li>
<li>
<p>グラフ解析は、ソーシャルネットワーク、創薬、レコメンデーションシステムなど、多くのアプリケーションで重要な役割を果たしています。グラフのサイズが大きく、メインメモリの容量を超える場合があるため、アプリケーションのパフォーマンスはストレージアクセス時間によって制限されます。アウトオブコアのグラフ処理フレームワークは、グラフシャーディングやサブグラフ分割などの技術を用いて、このストレージアクセスのボトルネックに対処しようとしています。これらの技術を用いても、異なるグラフシャードやサブグラフにまたがってデータにアクセスする必要があるため、ストレージシステムの性能に大きな障害となっています。この論文では、SSD上で保存、アクセス、グラフ解析を実行するための完全なシステムソリューションであるGraphSSDと呼ばれる、グラフセマンティックを意識したソリッドステートドライブ（SSD）フレームワークを提案します。ストレージをブロックの集合として扱うのではなく、GraphSSDはグラフのレイアウト、アクセス、更新メカニズムを決定しながら、グラフ構造を考慮します。GraphSSDは、SSDの従来の論理ページと物理ページのマッピングメカニズムを、新しい頂点間マッピングスキームに置き換え、フラッシュの特性に関する詳細な知識を活用してページアクセスを最小限に抑えます。また、GraphSSDは、不必要なページ移動のオーバーヘッドを最小限に抑えることで、効率的なグラフ更新（頂点とエッジの変更）をサポートします。GraphSSDは、アプリケーション開発者がアプリケーション内でネイティブデータとしてグラフにアクセスできるようにするシンプルなプログラミング・インターフェースを提供し、コード開発を簡素化します。また、グラフアクセスAPIを適切なストレージアクセスメカニズムにマッピングするための最小限の変更で、NVMe（不揮発性メモリエクスプレス）インターフェースを補強します。</p>
<p>評価の結果、GraphSSDフレームワークは、基本的なグラフデータ取得関数に対して最大1.85倍の性能向上を実現し、広く使われている幅優先探索、連結成分、ランダムウォーク、最大独立集合、ページランクのアプリケーションに対して、それぞれ平均1.40倍、1.42倍、1.60倍、1.56倍、1.29倍の性能向上を実現していることがわかりました。</p>
</li>
<li>
<p>CROW: a low-cost substrate for improving DRAM performance, energy efficiency, and reliability</p>
</li>
<li>
<p>DRAM は、数十年前からメインメモリを設計する際の主要な技術となっています。最近では、マルチコアシステムの設計や大容量データセットへの応用が進んでおり、システムの重要なボトルネックとしてのDRAMの役割はますます大きくなっています。我々は、DRAM の性能、エネルギー効率、信頼性を向上させるための新しいメカニズムを可能にする柔軟性のある基板、CROW (Copy-Row DRAM) を提案しています。CROW基板を用いて、1)頻繁にアクセスされる行へのDRAM活性化レイテンシを38%低減する低コストのDRAM内キャッシングメカニズムと、2)DRAMリフレッシュ動作の性能とエネルギーオーバーヘッドを軽減するために、短い保持時間の行を使用しないメカニズムを実装しています。CROWの柔軟性により、両方のメカニズムを同時に実装することができます。当社の評価によると、ある特定の実装では、2つのメカニズムが相乗的にシステム性能を20.0%向上させ、メモリ集約型の4コアワークロードでDRAMエネルギーを22.3%削減することがわかりました。</p>
</li>
<li>
<p>Janus: optimizing memory and storage support for non-volatile memory systems</p>
</li>
<li>
<p>不揮発性メモリ（NVM）テクノロジーは、メモリ内の永続的なデータを直接操作することができます。永続的データのクラッシュ一貫性を確保することは、データ更新がNVMに至るまで到達することを強制するため、これらの書き込み要求はクリティカルパス上に置かれることになります。最近の文献では、このパフォーマンスへの影響を軽減することが求められています。しかし、これまでの研究では、NVMで永続的なデータを維持するために必要なメモリコントローラで実行されるすべてのバックエンドメモリ操作（BMO）を十分に考慮していませんでした。これらのBMOには、セキュリティ、耐久性、寿命保証を提供するために必要な暗号化、完全性保護、圧縮、重複排除などのサポートが含まれています。これらの BMO は、NVM の書き込みレイテンシを大幅に増加させ、クリティカルな書き込み要求に起因する性能低下を悪化させる。この作業の目標は、NVMシステムにおける書き込み要求のBMOオーバーヘッドを最小化することです。</p>
<p>中心的な課題は、これらの一見依存的でモノリシックなBMOを最適化する方法を見出すことです。私たちの重要な洞察は、各BMOを一連のサブオペレーションに分解し、2つのメカニズムによって全体的なレイテンシを削減することです。(i) BMO間でのサブオペレーションの並列化と、(ii) サブオペレーションの入力が準備でき次第、クリティカルパスからサブオペレーションを事前に実行する。本論文では、一般的なクラッシュ一貫性のあるプログラミングモデルや様々なBMOと互換性のある事前実行要求を発行するために使用できる汎用的なソフトウェアインターフェースを公開する。これらの考えに基づいて、我々は、NVMシステムにおけるBMOの並列化と事前実行を行うハードウェアとソフトウェアの協調設計であるJanus1を提案する。暗号化、完全性検証、重複排除を統合したNVMシステムでJanusを評価し、提案するソフトウェアインタフェースを介して、手動または自動コンパイラパスを用いて事前実行要求を発行する。これらの処理を連続的に実行するシステムと比較して、手動では2.35倍、自動では2.00倍の高速化を達成した。</p>
</li>
<li>
<p>Anubis: ultra-low overhead and recovery time for secure non-volatile memories</p>
</li>
<li>
<p>セキュアな不揮発性メモリ（NVM）の実装は、主にデータと一緒にセキュリティ・メタデータを永続化する必要があるため、困難を極めます。従来のセキュア・メモリとは異なり、NVM を搭載したシステムはクラッシュ後にデータを回復することが期待されているため、セキュリティ・メタデータも回復可能でなければなりません。これまでの研究では、暗号化カウンタのリカバリを検討してきましたが、完全性で保護されたシステムのリカバリに焦点を当てた研究はほとんど行われていませんでした。特に、Merkle Tree をどのようにして復元するかという点に注目しています。このためには、2 つの大きな課題があると考えられます。第一に、並列化可能な整合性ツリー、例えばインテルのSGXツリーを回復するには、レベル間依存性のために非常に特殊な処理が必要です。第二に、実用的なNVMサイズ（テラバイトが想定される）のリカバリ時間は数時間かかる。ほとんどのデータセンター、クラウドシステム、断続的な電力デバイス、さらにはパソコンでさえも、電源復旧後はほぼ瞬時に復旧すると予想されています。実際、これはNVMの大きな約束事の一つである。</p>
<p>本論文では、復旧時間を約107倍（8時間からわずか0.03秒）に高速化するハードウェアのみのソリューション「Anubis」を提案します。さらに、IntelのSGXのように、レベル間依存ツリーを回復するための斬新でエレガントな方法を提案しています。最も重要なことは、最も困難な完全性保護スキームの一つである完全性保護スキームの回復性を確保する一方で、Anubisは、一般的なメルクル木を用いたシステムの回復に何時間もかかり、SGXスタイルの木の回復に失敗する最先端のスキームであるOsirisよりもわずか2%高いパフォーマンスオーバーヘッドを発生させることです。</p>
</li>
</ul>
<h3 id="session-2b-gpus">Session 2B: GPUs</h3>
<ul>
<li>
<p>Emerald: graphics modeling for SoC systems</p>
</li>
<li>
<p>モバイル・システム・オン・チップ（SoC）は、ユビキタス・コンピューティング・プラットフォー ムとなり、近年ではますます異種かつ複雑になってきています。典型的な SoC には、CPU、グラフィックス・プロセッサ・ユニット（GPU）、画像プロセッサ、ビデオエンコーダー/デコーダー、AI エンジン、デジタル・シグナル・プロセッサ（DSP）、2D エンジンなどが含まれます [33, 70, 71]。オフチップメモリ帯域幅とSoCダイエリアの両方の観点から、最も重要なSoCユニットの1つがGPUである。この論文では、グラフィックスとGPGPUアプリケーションのための統一モデルを提供するために、既存のツールをベースに構築されたシミュレータであるEmeraldを紹介します。Emeraldは、OpenGL(v4.5)とOpenGL ES(v3.2)のシェーダをGPGPU-Simのタイミングモデル上で実行できるようにし、gem5とAndroidと統合してフルSoCをシミュレートします。このようにして、Emeraldは、グラフィックスの影響を含めたシステムレベルのSoCの相互作用を研究するためのプラットフォームを提供します。</p>
<p>ここでは、Emeraldを用いた2つのケーススタディを紹介します。1つ目は、Emeraldのフルシステムモードを使って、SoCシステムのメモリ構成とスケジューリングスキームを研究・分析することで、システム全体の相互作用の重要性を強調しています。もう一つは、エメラルドのスタンドアロンモードを用いて、各GPUコアに割り当てられたグラフィックスシェーディング作業のバランスをとるための新しいメカニズムを評価しています。</p>
</li>
<li>
<p>MGPUSim: enabling multi-GPU performance modeling and optimization</p>
</li>
<li>
<p>データ並列ワークロードの急速な普及と規模の拡大に伴い、グラフィック・プロセッシング・ユニット(GPU)の生の計算能力も増加しています。シングルGPUプラットフォームではこれらの性能要求を満たすのに苦労しているため、マルチGPUプラットフォームがハイパフォーマンスコンピューティングの世界を支配し始めています。このようなシステムの出現は、GPU マイクロアーキテクチャ、マルチ GPU インターコネクトファブリック、ランタイムライブラリ、および関連するプログラミングモデルなど、多くの設計上の課題を提起しています。研究コミュニティには、次世代のマルチGPUシステム設計を評価するための、一般に公開されている包括的なマルチGPUシミュレーションフレームワークが、現在のところ不足しています。</p>
<p>本研究では、AMDのグラフィックス・コア・ネクスト3(GCN3)命令セット・アーキテクチャをベースにした、サイクル精度が高く、広範囲に検証されたマルチGPUシミュレータであるMGPUSimを紹介します。MGPUSimは、マルチスレッド実行を内蔵しており、高速で並列化された正確なシミュレーションを可能にします。性能精度の面では、MGPUSimは実際のGPUハードウェアと平均5.5%の差しかありません。また，4コアCPUで機能エミュレーションを3.5倍，詳細タイミングシミュレーションを2.5倍の平均速度で実行し，シリアルシミュレーションと同等の精度を実現しています．</p>
<p>2つの具体的な設計スタディを通して，シミュレータの柔軟性と能力を示しています．1 つ目の設計研究では，GPU プログラマがマルチ GPU プログラミングの複雑さを回避しつつ，マルチ GPU メモリ内のデータ配置を正確に制御できるようにするための API 拡張機能である Locality API を提案しています．第二の設計研究では、ハードウェアがデータ配置を段階的に改善することを可能にするカスタマイズされたマルチGPUメモリ管理システムである<u>P</u>rogressive P<u>a</u>ge <u>S</u>plitting M<u>i</u>gration(PASI)を提案します。ディスクリート4GPUシステムの場合、Locality APIを利用することで、統一された4GPUプラットフォームと比較して、1.6倍(幾何学的平均値)の高速化が可能であり、PASIを利用することで、全てのベンチマークにおいて、2.6倍(幾何学的平均値)の性能向上が可能であることを確認した。</p>
</li>
<li>
<p>Linebacker: preserving victim cache lines in idle register files of GPUs</p>
</li>
<li>
<p>最新の GPU は、同時実行中の数十個のワープで共有されるキャッシュサイズが限られているため、キャッシュ競合に悩まされています。ワープごとのキャッシュサイズを増やすために、以前の技術では、アクティブなワープの数を制限するワープスロットリングが提案されています。ワープのスロットリングでは、ワープがスロットリングされるたびに、いくつかのレジスタが動的に使用されなくなります。GPU における厳しいキャッシュサイズの制限を考慮して、本研究では Linebacker (LB) と呼ばれる新しいキャッシュ管理技術を提案します。CTAが非アクティブになると、ラインバッカーはスロットルされたCTAのレジスタをオフチップメモリにバックアップします。その後、ラインバッカーは対応するレジスタファイル空間を犠牲者キャッシュ空間として使用します。ロード命令が犠牲キャッシュラインにデータを見つけた場合、そのデータは単純なレジスタ・レジスタ移動操作によって直接宛先レジスタにコピーされます。犠牲者キャッシュラインバッカーの効率をさらに向上させるために、犠牲者キャッシュスペースは、データの局所性が高い少数のロード命令にのみ割り当てられます。犠牲者キャッシュのインデキシングと管理スキームの慎重な設計により、ラインバッカーは以前に提案されたワープ・スロットリング技術と比較して29.0%の高速化を実現しました。</p>
</li>
<li>
<p>Opportunistic computing in GPU architectures</p>
</li>
<li>
<p>計算コアとメモリ階層間のデータ転送のオーバーヘッドは、フォン・ノイマン・アーキテクチャの永続的な問題であり、この問題は、メニーコア・システムの出現によって、より困難になってきています。このオーバーヘッドを軽減するための概念的に強力なアプローチは、ニアデータコンピューティング(NDC)として知られる計算をデータに近づけることです。最近では、CPUベースのマルチコアシステムではNDCが様々な形で研究されていますが、GPUドメインではあまり注目されていませんでした。本論文では，計算コアとLLC（Last-Level Cache）間のオンチップデータ転送を最小化することを目的とした，GPUアーキテクチャのための新しいNDCソリューションを提示する．これを達成するために，まず，GPU アプリケーションで頻繁に発生する Load-Compute-Store 命令チェーンを特定します．これらのチェーンは、データが存在する場所に近い計算ユニットにオフロードされると、データの移動を大幅に減らすことができます。我々はLLC-ComputeとOmni-Computeと呼ばれる2つのオフロード技術を開発しました。最初の手法であるLLC-Computeは、LLCにオフロードされた計算を処理するための計算ハードウェアをLLCに追加します。もう一つの技術（Omni-Compute）では，他のGPUコアがオフロードした命令をGPUコアが計算できるようにするために，単純な簿記ハードウェアを採用している．9 つの GPGPU ワークロードでの実験的評価によると，LLC-Compute 技術はベースライン GPU 設計と比較して，平均で 19%の性能向上（IPC），11%の性能向上/ワットの向上，およびオンチップデータ移動の 29%の削減を実現しました．Omni-Compute設計では、これらの利点がそれぞれ31%、16%、44%にまで向上しています。</p>
</li>
<li>
<p>Interplay between hardware prefetcher and page eviction policy in CPU-GPU unified virtual memory</p>
</li>
<li>
<p>GPGPUのメモリ容量は、メモリ要件が増大し続けるデータ集約型アプリケーションにとって大きな課題となっています。限られたGPUメモリ空間にワークロードを収めるためには、プログラマーが手作業で作業セットをタイリングしてワークロードを分割し、ユーザーレベルのデータ移行を実行する必要があります。この負担からプログラマーを解放するために、ユーザーに透過的なオンデマンドのページングとマイグレーションをサポートするユニファイド・バーチャル・メモリ（UVM）が開発されました。さらに、GPU メモリが過剰に供給されている状況で自動的にページ交換を実行することで、メモリの過剰供給の問題を解決します。しかし、ページフォルトのナイーブな処理は、パフォーマンスの桁違いの低下を引き起こす可能性があることがわかりました。さらに，CPU から GPU へのデータのプリフェッチはページフォルトのレイテンシを隠すことができますが，さまざまなプリフェッチ機構の違いにより，性能が大幅に異なる結果になることがわかりました．このため、PCI-e 3.0 16x を搭載した GeForceGTX 1080ti GPU で広範な実験を行い、GPU メモリのロカリティを向上させる効果的なプリフェッチ メカニズムが存在することを発見しました。しかし、GPU メモリがその容量まで満たされると、そのようなプリフェッチ機構は、ロカ リティを認識しない退避ポリシーのために、すぐに逆効果であることが判明します。このため、ハードウェアプリフェッチャのセマンティクスを意識した新しい退避ポリシーの設計が必要となる。我々は、既存のハードウェアプリフェッチャの仕組みを活用し、追加の実装と性能のオーバーヘッドを発生させない、プログラマに依存しない、ロカリティを意識した2つの新しいプリイベクションポリシーを提案する。提案するツリーベースのプリエビングポリシーとハードウェアプリフェッチャを組み合わせることで、LRUベースの4KBと2MBのページ置換戦略と比較して、それぞれ平均93%と18.5%の性能向上を実現することを実証した。さらに、達成された性能の高速化を分析するために、検討中のGPUワークロードのメモリアクセスパターンを調査した。</p>
</li>
</ul>
<h3 id="session-3a-deep-learning-and-ai">Session 3A: Deep Learning and AI</h3>
<ul>
<li>
<p>Sparse ReRAM engine: joint exploration of activation and weight sparsity in compressed neural networks</p>
</li>
<li>
<p>非効率的な計算を減らすためにモデルのスパリティを利用することは、DNN推論アクセラレータのエネルギー効率を達成するために一般的に使用されているアプローチである。しかし、クロスバー構造が密に結合されているため、ReRAMベースのNNアクセラレータにスパリティを利用することはあまり検討されていません。ReRAMベースのNNアクセラレータの既存のアーキテクチャ研究では、クロスバーアレイ全体を1サイクルで起動できることを前提としています。しかし、推論精度を考慮すると、実際には、行列ベクトル計算はオペレーションユニット（OU）と呼ばれるより小さな粒度で実行されなければなりません。OUベースのアーキテクチャは、DNNのスパースパリティを利用する新たな機会を創出する。本論文では、重みと活性化の両方のスパースパリティを利用した最初の実用的なスパースReRAMエンジンを提案する。我々の評価では、提案手法が非効率な計算を排除するのに有効であり、大幅な性能向上と省エネを実現していることが示されている。</p>
</li>
<li>
<p>MnnFast: a fast and scalable system architecture for memory-augmented neural networks</p>
</li>
<li>
<p>メモリ・アグメンテッド・ニューラルネットワークは、メモリに記憶された過去の履歴から推論を行うことができるため、多くの研究者の間で注目されています。特に、このようなメモリ拡張型ニューラルネットワークの中でも、メモリネットワークは、他のネットワークに比べて膨大な推論力を持ち、大量の入力から学習することができることで知られています。また、入力データセットのサイズが急速に大きくなるにつれて、大規模なメモリネットワークの必要性が絶えず生じている。このような大規模メモリネットワークは優れた推論能力を持つが、現在の計算機インフラではシステムアーキテクチャが限られているため、スケーラブルな性能を発揮することができない。</p>
<p>本論文では、高速かつスケーラブルな推論性能を実現するために、大規模メモリネットワークのための新しいシステムアーキテクチャであるMnnFastを提案する。本論文では、大規模メモリネットワークの高速化とスケーラブルな推論性能を実現するための新しいシステムアーキテクチャであるMnnFastを提案する。その結果、現在のアーキテクチャは、メモリ帯域の消費量が多い、計算量が多い、キャッシュ競合が多い、という3つの大きな性能問題を抱えていることがわかった。これらの性能問題を克服するために、我々は3つの新しい最適化を提案する。第一に，メモリ帯域幅の消費を低減するために，データ流出のサイズを最小化し，オフチップメモリアクセスのオーバーヘッドの大部分を隠すストリーミングを用いた新しいカラムベースアルゴリズムを提案する．第二に、高い計算オーバーヘッドを低減するために、大量の出力計算をバイパスするゼロスキップ最適化を提案する。最後に、キャッシュ競合を排除するために、埋め込み行列を効率的にキャッシュするための埋め込みキャッシュを提案する。</p>
<p>評価の結果，MnnFastは様々な種類のハードウェアで有効であることが示された．また，CPU，GPU，FPGAの各ハードウェアにおいて，MnnFastの有効性が確認された．MnnFastはCPUで最大5.38×、GPUで最大4.34×、FPGAで最大2.01×のスループット向上を実現しています。また、CPUベースのMnnFastと比較して、FPGAベースのMnnFastは6.54倍のエネルギー効率を実現しています。</p>
</li>
<li>
<p>TIE: energy-efficient tensor train-based inference engine for deep neural network</p>
</li>
<li>
<p>人工知能（AI）の時代には、ディープニューラルネットワーク（DNN）が最も重要かつ強力なAI技術として登場しています。しかし、大規模なDNNモデルはストレージと計算量の両方を必要とするため、リソースに制約のあるシナリオでDNNを採用するには大きな課題があります。そのため、モデル圧縮はDNNを広く普及させるための重要な技術となる。</p>
<p>本論文では、アーキテクチャ領域において非常に有望な圧縮技術であるテンソル列車(TT)分解を考慮することで、最先端の技術を発展させた。この手法の特徴は、非常に高い圧縮率である。しかし、TT形式のDNNモデルに対する推論では、大量の冗長計算が発生し、エネルギー消費が大きくなるという課題がある。そのため、TT分解を簡単に適用することはできません。</p>
<p>本論文では、この根本的な課題を解決するために、TT-format DNNのための計算効率の高い推論スキームを開発した。本スキームは、1)理論上の限界である乗算数を達成し、冗長な計算をすべて排除することができる、2)多段処理スキームにより、すべてのテンソルコアへの集中的なメモリアクセスを削減し、大幅な省エネルギーを実現することができる、という2つの利点を持つ。</p>
<p>この新しい推論スキームに基づいて、TT形式の圧縮DNN推論エンジンであるTIEを開発した。TIEは柔軟性が高く、様々なニーズに応じて様々なタイプのネットワークをサポートします。TIEのプロトタイプは、CMOS 28nm技術を用いて16個の処理素子（PE）を実装しています。1000MHzで動作するTIEアクセラレータの消費電力は1.74mm2、消費電力は154.8mW。EIEと比較して、TIEは、異なるワークロードにおいて、それぞれ面積効率が7.22倍～10.66倍、エネルギー効率が3.03倍～4.48倍向上しています。CirCNNと比較すると、TIEはスループットが5.96倍、エネルギー効率が4.56倍向上しています。この結果は、TIEが最先端のソリューションと比較して大きな優位性を示していることを示しています。</p>
</li>
<li>
<p>Accelerating distributed reinforcement learning with in-switch computing</p>
</li>
<li>
<p>強化学習(RL)は、新しいAIベースのアプリケーションが環境の変化にインテリジェントに反応する能力を要求していることから、最近注目を集めています。分散型ディープニューラルネットワーク(DNN)訓練とは異なり、分散型RL訓練には独特の作業負荷特性があり、それは、はるかに小さいサイズでありながら、より頻繁な勾配集約で、桁違いに多くの反復を生成することです。より具体的には、典型的なRLアルゴリズムを用いた我々の研究では、その分散訓練はレイテンシが重要であり、勾配集約のためのネットワーク通信は、各訓練反復の実行時間の最大83.2%を占めることが示されている。</p>
<p>この論文では、グラデーションアグリゲーションをサーバノードからネットワークスイッチに移動させるスイッチ内高速化ソリューションであるiSwitchを紹介する。これにより、同期学習におけるエンドツーエンドのネットワークレイテンシが減少するだけでなく、非同期学習における重み更新の高速化により収束性が向上する。さらに、スイッチ内アクセラレータを用いて、勾配ベクトルではなく、ネットワークパケットの粒度でオンザフライの勾配集約を行うことで、同期学習のオーバーヘッドをさらに削減することができる。さらに、分散RL学習アルゴリズムを再考し、ラックスケールでの分散RL学習の並列性とスケーラビリティをさらに向上させるための階層型アグリゲーション機構を提案する。</p>
<p>実世界のプログラマブル・スイッチNetFPGAボードを用いてiSwitchを実装しています。プログラマブル・スイッチの制御およびデータ・プレーンを拡張し、通常のネットワーク機能に影響を与えることなくiSwitchをサポートしています。最先端の分散型学習アプローチと比較して、iSwitchはシステムレベルで同期型分散型学習で最大3.66倍、非同期型分散型学習で最大3.71倍の高速化を実現すると同時に、より優れたスケーラビリティを実現しています。</p>
</li>
<li>
<p>Eager pruning: algorithm and architecture support for fast training of deep neural networks</p>
</li>
<li>
<p>強化学習(RL)は、新しいAIベースのアプリケーションでは、環境の変化にインテリジェントに反応する能力が求められるため、最近注目を集めています。分散型ディープニューラルネットワーク(DNN)学習とは異なり、分散型RL学習には独特の作業負荷特性があります。より具体的には、典型的なRLアルゴリズムを用いた我々の研究では、分散学習ではレイテンシが重要であり、勾配集約のためのネットワーク通信は、各学習反復の実行時間の最大83.2%を占めることが示されている。</p>
<p>本稿では、グラディエントアグリゲーションをサーバノードからネットワークスイッチに移動させるスイッチ内アクセラレーションソリューションであるiSwitchを紹介する。これにより、同期学習におけるエンドツーエンドのネットワークレイテンシを低減するだけでなく、非同期学習における重みの更新を高速化することで収束性を向上させることができる。さらに、スイッチ内アクセラレータを使用して、勾配ベクトルではなくネットワークパケットの粒度でオンザフライの勾配集約を実行することで、同期学習のオーバーヘッドをさらに削減することができる。さらに、分散型RL学習アルゴリズムを再考し、ラックスケールでの分散型RL学習の並列性とスケーラビリティをさらに向上させるための階層型アグリゲーション機構を提案します。</p>
<p>iSwitchは、実世界のプログラマブル・スイッチNetFPGAボードを使用して実装されています。これは、プログラマブル・スイッチの制御およびデータ・プレーンを拡張し、通常のネットワーク機能に影響を与えることなくiSwitchをサポートします。最先端の分散学習アプローチと比較して、iSwitchはシステムレベルで同期分散学習で最大3.66倍速、非同期分散学習で最大3.71倍速となり、同時に優れたスケーラビリティを実現しています。</p>
</li>
<li>
<p>Laconic deep learning inference acceleration</p>
</li>
<li>
<p>我々は、ディープラーニングモデルを用いて推論中の非効率的な計算を透過的に識別する方法を提示する。具体的には、乗算をビットレベルまで分解することで、幅広い選択のニューラルネットワーク(8bと16b)において、推論中の乗算に必要な作業量を少なくとも40倍削減できる可能性がある。この方法は、数値的には同じ結果をもたらし、全体的な精度には影響を与えない。我々は、ディープラーニングネットワークを用いた推論のエネルギー効率を高めるために、このアプローチを実装したハードウェアアクセラレータであるLaconicを紹介する。Laconicは、作業削減の可能性を慎重に放棄して、低コストでシンプルかつエネルギー効率の高い設計を実現し、他の最先端のアクセラレータ（最適化されたDaDianNaoのような設計[13]、Eyeriss[15]、SCNN[71]、Pragmatic[3]、BitFusion[83]）を凌駕しています。我々は、16b、8b、および1b/2b固定小数点量子化モデルを研究している。</p>
</li>
</ul>
<h3 id="session-3b-security">Session 3B: Security</h3>
<ul>
<li>
<p>MicroScope: enabling microarchitectural replay attacks</p>
</li>
<li>
<p>ハードウェアベースのTrusted Execution Environments(TEE)の人気は、最近、IntelのSoftware Guard Extensions(SGX)の導入により急上昇している。SGXでは、ユーザー・プロセスは、エンクレイブと呼ばれる隔離された実行環境を介して、オペレーティング・システムなどのスーパーバイザ・ソフトウェアから保護されます。TEE によって提供される分離保証にもかかわらず、その防御メカニズムを迂回する多数のマイクロアーキテクチャサイドチャネル攻撃が実証されています。しかし、防御者にとってすべての希望が失われているわけではありません。現代の多くの微細で高解像度のサイドチャネル（例えば、実行ユニットのポート競合）は、大量のノイズを発生させ、秘密を確実に抽出するための攻撃者のタスクを複雑にしています。</p>
<p>これにより、SGXの敵対者は、被害者にページ障害命令を繰り返し再生させることで、被害者の1回の実行でほぼ任意のマイクロアーキテクチャーサイドチャネルを非ノイズ化することができる。我々は、MicroScopeと呼ばれるフレームワークで我々のアイデアを設計、実装、実証し、悪名高いノイズの多いサイドチャンネルのノイズ除去に使用しています。主な結果は、MicroScopeが実行ユニットポートの競合チャネルをどのようにノイズ除去できるかを示しています。具体的には、犠牲となるプログラムの単一の論理実行において、Micro-Scopeがどのようにしてわずか2つの除算命令の有無を確実に検出できるかを示しています。このような攻撃は、個々の浮動小数点命令へのサブノーマル入力を検出したり、エンクレイブ境界で分岐予測器をフラッシュする今日の対策にもかかわらず、エンクレイブ内の分岐方向を推測したりするために使用できます。また、MicroScope を用いて、OpenSSL の AES 実装に対するキャッシュベースの攻撃をシングルステップ化し、非ノイズ化しています。最後に、マイクロアーキテクチャーによるリプレイ攻撃のより広範な意味合いについて議論し、リプレイを引き起こす可能性のある他のメカニズムについても議論します。</p>
</li>
<li>
<p>SecDir: a secure directory to defeat directory side-channel attacks</p>
</li>
<li>
<p>キャッシュコヒーレンスのためのディレクトリは、最近、競合ベースのサイドチャネル攻撃に対して脆弱であることが示されています。ディレクトリの競合を強制することで、攻撃者は被害者のディレクトリエントリを退避させることができ、その結果、プライベートキャッシュから被害者のキャッシュラインを退避させることができます。この証拠は、ディレクトリをセキュリティのために再設計する必要があることを強く示唆しています。安全なディレクトリの鍵は、プロセス間の干渉をブロックすることです。悲しいことに、多くのコアを持つ環境では、これを実行するのは難しいか、コストがかかります。</p>
<p>この論文では、スケーラブルなセキュアディレクトリの最初の設計を紹介します。我々はこれをSecDirと呼んでいる。SecDirは、従来のディレクトリで使用されていたストレージの一部を、Victim Directories (VD)と呼ばれる犠牲者キャッシュの方法で使用されるコアごとのプライベートディレクトリ領域に再割り当てします。VDのパーティション化された性質は、コア間のディレクトリ干渉を防ぎ、ディレクトリのサイドチャネル攻撃を打ち破ります。コアのVDは分散されており、コアのプライベートL2キャッシュの行数と同じ数のエントリを保持します。攻撃時のVD内での犠牲者の自己矛盾を最小限に抑えるために、VDはカッコーディレクトリとして構成されています。このような設計では、攻撃者から被害者の衝突パターンが見えなくなります。評価のために、Intel Skylake-Xサーバのディレクトリをモデル化し、SecDirの有無をシミュレーションしました。その結果、SecDirは無視できるほどのパフォーマンスオーバーヘッドを持っていることがわかりました。さらに、SecDirは面積効率に優れています。</p>
</li>
<li>
<p>Secure TLBs</p>
</li>
<li>
<p>本論文では、最新のプロセッサにおける新たな攻撃ベクトル、すなわち、TLB（Translation Look-aside Buffers）に起因するタイミングベースのサイド攻撃と隠蔽チャネル攻撃に焦点を当てている。この論文では、まず、TLB のタイミングベースの脆弱性を網羅的に列挙するための新しい 3 段階のモデル化アプローチを提示する。この3段階モデルを基に、TLBの脆弱性をテストするマイクロセキュリティベンチマークを自動的に生成する方法を示します。標準的な TLB の安全性を示した後、2 つの新しい安全な TLB 設計を提示します：スタティック・パーティション（SP）TLB とランダム・フィル（RF）TLB です。これらの新しい安全な TLB は、RISC-V プロセッサアーキテクチャの Rocket Core 実装を用いて評価されています。また、3段階モデルとセキュリティベンチマークを用いて、新設計の安全性をシミュレーションで解析した。その結果、提案された安全なTLBは、これまでに公表された攻撃だけでなく、新しい3段階モデルを用いて発見されたTLBにおける他の新しいタイミングベースの攻撃に対しても防御することができることがわかった。性能オーバーヘッドをFPGAベースのセットアップで評価し、例えば、RF TLBがすべての攻撃を防御しながら10%未満のオーバーヘッドを持つことを示している。</p>
</li>
<li>
<p>New attacks and defense for encrypted-address cache</p>
</li>
<li>
<p>コンフリクトベースのキャッシュ攻撃は、キャッシュの競合を介して退避をオーケストレーションすることで、敵対者が共同実行中のアプリケーションのアクセスパターンを推測することを可能にします。 このような攻撃は、キャッシュ内のラインの位置をランダム化することで緩和することができます。 最近提案したCEASERは、暗号化されたアドレスを用いてキャッシュにアクセスし、定期的に暗号化キーを変更することで、キャッシュのランダム化を実用的なものにしています。 CEASERを退避集合の形成に関する最先端のアルゴリズムで解析した結果、リマップレート1%のCEASERであれば、何年もの攻撃に耐えることができることがわかった。</p>
<p>本論文では、退避集合の形成における最先端を大きく押し上げる2つの新しい攻撃を提示する。 我々の最初の攻撃は、退避集合の形成に必要な時間をO(L2)からO(L)に短縮する。 この攻撃は、最もよく知られている攻撃よりも35倍速く、CEASERのリマップレートを35%に上げる必要があります。 我々の2番目の攻撃は、置換ポリシー(LRU, RRIP, Randomを解析)を悪用して迅速に退避セットを形成し、CEASERのリマップ率を100%以上に高める必要があり、非実用的なオーバーヘッドを発生させる。</p>
<p>これらの攻撃に対するCEASERのロバスト性を実用的に向上させるために、我々はキャッシュウェイを複数のパーティションに分割し、各パーティション内の異なるセットにキャッシュラインをマッピングするSkewed-CEASER(CEASER-S)を提案する。  この設計は、攻撃者が複数の可能な場所からキャッシュラインを取り除くことができる退避セットを形成しなければならないため、CEASERのロバスト性を大幅に向上させる。  我々は、CEASER-Sが1%のリマップ率を維持しながら、何年にもわたる攻撃に耐えられることを示している。  CEASER-Sは、無視できるほどのスローダウン(1%以内)と、新たに追加された構造体のための100バイト以下のストレージオーバーヘッドを発生させる。 </p>
</li>
<li>
<p>InvisiPage: oblivious demand paging for secure enclaves</p>
</li>
<li>
<p>Intel SGX のような最新のセキュアプロセッサは、悪意のある OS が偽のページフォルトを誘発し、そこからアプリケーションの秘密を推測するページフォルトチャネルを介して、アプリケーションのページレベルのアドレストレースが漏洩する脆弱性が残っています。この脆弱性を修正した従来の研究では、OSの要求ページングを無視するようには規定されていない。本研究では、ページフォルトチャネルを難読化すると同時に、OSの要求ページングを忘却させるInvisiPageを提案します。これを実現するために，InvisiPageはまず，アプリケーションとOSの間でページ管理アクションを慎重に分散させる．次に、InvisiPageは、ページ管理のためにカスタマイズされたOblivious RAM (ORAM)から派生した新しい構成を使用して、アプリケーションのページ管理とOSとの相互作用を保護します。最後に、新しいメモリ・パーティションを介してOSとのページ管理のやりとりを減らすことで、我々のアプローチのオーバーヘッドを下げています。機密性の高いデータを処理する一連のクラウド・アプリケーションについて、ページ・フォルト・チャネルに対処しながら、低オーバーヘッドでオブリビアス・デマンド・ページングを可能にすることを示しています。</p>
</li>
<li>
<p>TWiCe: preventing row-hammering by exploiting time window counters</p>
</li>
<li>
<p>DRAM を使用するコンピュータシステムは、DRAM 行に直接アクセスせずに、隣接する行を頻繁にアクティブにすることで、DRAM 行内のデータを反転させることができる行ハンマ ー（RH）攻撃にさらされています。RH 攻撃を防止するための提案は数多くありますが、これらの提案は、面積オーバーヘッドが大きく、敵対的なメモリアクセスパターンに対して顕著な性能低下をもたらすか、攻撃を検出する機能を持たない確率的な防御を提供するかのいずれかになります。</p>
<p>本論文では、Time Window Counter (TWiCe)をベースとした行リフレッシュと呼ばれる新しいカウンタベースのRH防止ソリューションを提案する。まず、RH を引き起こす可能性のある行の数は、行の活性化頻度と DRAM セルの保持時間の最大値によって制限されるという重要な観測を行いました。TWiCe が強力な決定論的保証で RH を防止する、DRAM バンクごとに必要なカウンターエントリの最大数を計算します。擬似連想キャッシュ設計を活用し、TWiCe テーブルを分離することで、面積とエネルギーオーバーヘッドをさらに削減しています。TWiCe は通常の DRAM オペレーションではパフォーマンスオーバヘッドを発生させず、最新の DRAM デバイスでは 0.7% 未満の面積とエネルギーオーバヘッドを発生させます。当社の評価では、RH 攻撃シナリオを含む敵対的なメモリアクセスパターンにおいて、TWiCe は 0.006% 未満の追加 DRAM 行活性化を行わないことが示されています。</p>
</li>
</ul>
<h3 id="session-4a-caching-and-data-management">Session 4A: Caching and Data Management</h3>
<ul>
<li>
<p>Duality cache for data parallel acceleration</p>
</li>
<li>
<p>Duality Cacheは、汎用データ並列アプリケーションをキャッシュ上で実行することを可能にするインキャッシュ計算アーキテクチャである。本稿では，インキャッシュ浮動小数点演算や超越関数の実行，データ並列実行モデルの実現，既存のCUDAプログラムを受け入れるコンパイラの設計，様々なワークロード特性への柔軟な対応などの技術を用いて，Duality Cacheシステムスタックを構築する全体的なアプローチを紹介する．</p>
<p>Duality Cache アーキテクチャに存在する大規模並列化を適用することで，サーバクラスの GPU ベンチマークで 3.6 倍，OpenACC ベンチマークで 4.0 倍の性能向上を実現した．既存のキャッシュを再利用することで、わずか3.5%の面積コストでCPUの性能が72.6倍向上します。Duality Cacheは、GPUで5.8倍、CPUで21倍のエネルギーを削減します。</p>
</li>
<li>
<p>Adaptive memory-side last-level GPU caching</p>
</li>
<li>
<p>新興のGPUアプリケーションはますます高い計算要求を示しており、これによりGPUメーカーはますます多くのストリーミング・マルチプロセッサ（SM）を搭載したGPUを構築するようになりました。高帯域幅でSMにデータを提供することは、メモリ階層とネットワーク・オン・チップ（NoC）に大きな圧力をかけます。現在のGPUは通常、メモリ側の最終レベルキャッシュ(LLC)をすべてのSMで共有される等サイズのスライスに分割しています。共有LLCの方が一般的にミスレートは低くなりますが、SM間でのデータ共有度が高いワークロードでは、プライベートLLCの方が、異なるLLCスライス間で複製されるキャッシュラインの帯域幅が増加するため、パフォーマンスに大きな優位性があることがわかりました。</p>
<p>この論文では，読み取り専用の共有データに高い帯域幅を必要とする共有集約的なワークロードのパフォーマンスを向上させるために，適応的なメモリ側ラストレベルGPUキャッシングを提案しています．アダプティブ・キャッシングは，プライベート・キャッシングの下でのミス・レートの増加とLLC帯域幅の増加のバランスをとる軽量なパフォーマンス・モデルを活用しています．共有集約的なワークロードのパフォーマンスを向上させるだけでなく，アダプティブキャッシングは，LLC がプライベートキャッシュとして構成されている場合には，パワーゲーティングを行い，2 段目をバイパスすることで，（共同設計された）階層型 2 段クロスバー NoC のエネルギーを節約することができます．17 の GPU ワークロードを使用した実験結果によると，共有集約的なワークロードでは，アダプティブキャッシングは共有 LLC と比較して平均で 28.1%（最大 38.1%）性能を向上させることが示されています．さらに，アダプティブキャッシングは，プライベートキャッシュとして構成した場合，NoC のエネルギーを平均で 26.6% (最大 29.7%) 削減し，システム全体のエネルギーを平均で 6.1% (最大 27.2%) 削減します．最後に，GPUのNoC設計空間の探索を通じて，階層的な2段クロスバーが，同じ二分割帯域幅を持つフルクロスバーや集中クロスバーよりも電力効率と面積効率の両方で優れていることを実証しました．</p>
</li>
<li>
<p>SCU: a GPU stream compaction unit for graph processing</p>
</li>
<li>
<p>グラフ処理アルゴリズムは、機械学習やデータ分析などの分野で多くの新興アプリケーションの鍵を握っています。大規模グラフの処理は高度な並列性を示しますが、メモリアクセスパターンは非常に不規則である傾向があり、メモリ発散によるGPGPUの効率低下を引き起こします。この問題を改善するために、GPGPUアプリケーションでは、アルゴリズムの反復処理のたびにストリーム圧縮処理を行い、アクティブなノード/エッジのサブセットを抽出し、後続のステップでは圧縮されたデータセットで動作するようにしている。</p>
<p>我々は、GPGPUアーキテクチャがストリーム圧縮には非効率的であることを示し、このカーネルの要件に合わせてプログラム可能なストリーム圧縮ユニット(SCU)にこのタスクをオフロードすることを提案する。SCU は GPU に緊密に統合された小さなユニットで、アクティブなノードやエッジを効率的にメモリ内の圧縮された配列に収集します。アプリケーションはシンプルな API を通じてこれを利用できます。グラフベースのアルゴリズムの残りのステップは，GPUの大量の並列性を利用してGPUコア上で実行されますが，SCUで準備されたデータ上で動作し，より大きなメモリ合体を実現することで，より高い効率を実現します．また、SCUは圧縮処理の際に、繰り返し訪れたノードや既に訪れたノードのフィルタリングを行い、GPGPUの負荷を大幅に軽減するとともに、圧縮されたノード/エッジを順番に書き込むことで、メモリの発散を抑えてメモリ合体を向上させます。</p>
<p>本研究では、最新のGPGPUアーキテクチャを拡張し、様々なアプリケーションでの性能を評価した。その結果、高性能GPUシステムでは1.37倍、低消費電力GPUシステムでは2.32倍の高速化、84.7%と69%の省エネ、面積では3.3%と4.1%の増加を達成した。</p>
</li>
<li>
<p>Filter caching for free: the untapped potential of the store-buffer</p>
</li>
<li>
<p>最近のプロセッサは、ストアがミスしてもリタイアできるようにストアバッファを含んでおり、ストアミスレイテンシを隠しています。ストアバッファは（性能のために）大きくなければならず、（正確性のために）すべての負荷で検索される必要があるため、面積とエネルギーの両方でコストのかかる構造になっています。しかし、すべての負荷において、ストアバッファはL1とTLBと並行してプローブされ、ストアバッファの本質的なヒット率や、ストアバッファのヒットを予測してL1とTLBのプローブを無効にすることでエネルギーを節約できるかどうかを気にすることなく、ストアバッファはプローブされます。</p>
<p>本研究では、メモリに書き戻されたデータを統一されたストアキュー/バッファ/キャッシュにキャッシュし、L1/TLBプローブを回避してエネルギーを節約するためにヒットを予測する。ストアキュー/バッファ/キャッシュ間のエントリの割り当てを動的に調整することで、ストールを起こさずにほぼ最適な再利用を実現しています。これを効率的かつ低コストで実現するには、ストアの主要な特性を認識する必要があります：フリーキャッシュ（正確性を保つためにストアバッファに書き込まなければならないため、追加のデータ移動は必要ありません）、低コストのコヒーレンス（ストアバッファ内のローカルでダーティなデータの状態変化を追跡するだけでよいため）、フリーで正確なヒット予測（メモリ依存性予測器がスケジューリングのためにすでにこれを行っているため）です。</p>
<p>その結果、性能に影響を与えることなく、SPEC2006上でストアバッファのヒット率を向上させ、ストアバッファ/TLB/L1の動的エネルギーを11.8%(最大26.4%)削減することができました(平均IPC改善率は1.5%で、最大4.7%)。これらの改善のためのコストは、L1キャッシュ容量の0.2%増加(1行あたり1ビット)と、ストアバッファ内のテールポインタの追加1つである。</p>
</li>
</ul>
<h3 id="session-4b-datacenter-and-end-to-end-compute">Session 4B: Datacenter and End-to-End Compute</h3>
<ul>
<li>
<p>Efficient metadata management for irregular data prefetching</p>
</li>
<li>
<p>テンポラルプリフェッチャは、任意のメモリアクセスパターンをプリフェッチする可能性があるが、通常はDRAMに格納しなければならない大量のメタデータを必要とする。2013年には、Irregular Stream Buffer（ISB）が、このメタデータをチップ上にキャッシュし、その内容をTLBの内容と同期させることで暗黙的に管理する方法を示した。この論文では、このアプローチの非効率性を明らかにし、メタデータキャッシュをフィードするために単純なメタデータプリフェッチャを使用する新しいメタデータ管理スキームを提示します。その結果、トラフィックオーバヘッドとIPCの両方の点で最先端の技術を大幅に進歩させた時間的プリフェッチャであるマネージドISB(MISB)を実現しました。</p>
<p>シングルコアのワークロードには高精度の独自シミュレータを、マルチコアのワークロードにはChampSimシミュレータを使用して、SPEC CPU 2006とCloudSuiteベンチマークスイートのプログラムでMISBを評価しました。その結果、シングルコアのワークロードでは、理想的なSTMSでは10.6%、現実的なISBでは4.5%の性能向上が見られましたが、MISBでは22.7%の性能向上が見られました。また、MISBはオフチップトラフィックを大幅に削減します。SPECの場合、MISBの70%のトラフィックオーバーヘッドは、STMSの約5分の1（342%）、ISBの約6分の1（411%）です。4コアのマルチプログラムワークロードでは、理想化されたSTMSの13.6%に対し、MISBは27.5%の性能向上を実現しています。CloudSuiteでは、MISBは12.8%の性能向上（理想化STMSでは6.0%）、7倍のトラフィック削減（MISBでは83.5%、STMSでは572.3%）を実現しています。</p>
</li>
<li>
<p>AsmDB: understanding and mitigating front-end stalls in warehouse-scale computers</p>
</li>
<li>
<p>プライベートおよびパブリッククラウドのワークロードでは、大量の命令ワークセットがあるため、命令キャッシュのミスが頻繁に発生し、数百万ドル規模のコストが発生します。これまでの研究では、この問題の重要性が高まっていることが明らかになっていますが、これまでのところ、ミスがどこから来ているのか、また、ミスを改善するためにはどのような機会があるのかについての分析はほとんど行われていませんでした。この課題に対処するために、本論文では3つの貢献をします。第一に、フロントエンドのボトルネックを追跡する新しい常時稼働型のフリート全体監視システムである AsmDB の設計と展開を紹介します。AsmDBは、ハードウェアサポートを使用してバースト的な実行トレース、フリート全体の時間的・空間的サンプリング、および洗練されたオフラインのポスト処理を収集し、フルプログラムの動的制御フローグラフを構築します。第二に、実世界のオンラインサービスからのAsmDBデータの縦断的な分析に基づいて、フロントエンドのストールの発生源に関する2つの詳細な洞察を提示します。(1)ホットコードと一緒に導入されたコールドコードは、キャッシュの断片化が大きく、それに対応して多くの命令キャッシュミスを引き起こす。(2)従来のキャッシュロカリティ戦略やネクストラインプリフェッチ戦略に対応できない遠いブランチやコールは、キャッシュミスの大部分を占めている。第三に、これらの洞察をターゲットとした二つの最適化をプロトタイプ化します。フラグメンテーションに起因するミスに対しては、キャッシュミスに寄与する最もホットな関数の一つであるmemcmpに焦点を当て、きめ細かなレイアウト最適化がどのように大きな利益をもたらすかを示します。遠隔ジャンプのターゲットでのミスに対しては、ソフトウェアコードのプリフェッチのための新しいハードウェアサポートを提案し、静的なプログラムフロー解析と動的なミスプロファイルを組み合わせた新しいフィードバック指向コンパイラ最適化をプロトタイプ化し、いくつかの大規模なウェアハウススケールのワークロードで大きなメリットを実証します。先行研究を改良し、我々の提案は、効率的でスケーラブルな方法でソフトウェアを介してプリフェッチを行うことにより、侵襲的なハードウェアの変更を回避することができます。シミュレーションの結果、このようなアプローチは、無視できるほどのオーバーヘッドで最大96%の命令キャッシュのミスをなくすことができることを示しています。</p>
</li>
<li>
<p>Fine-grained warm water cooling for improving datacenter economy</p>
</li>
<li>
<p>データセンターの消費電力の増加に伴い、業界ではエネルギー効率を向上させるために水冷に力を入れています。温水を用いてサーバを冷却することは、冷却エネルギーを削減するための効率的な方法として考えられてきた。しかし，温水による冷却は，きめ細かい冷却制御ができないために，サーバ間の熱的不均衡が生じ，冷却不良のリスクやエネルギー効率の低下を招く可能性がある．本論文では、冷却のミスマッチにきめ細かく対処するために、水冷システムに熱電冷却器を組み込むハイブリッド冷却アーキテクチャの設計を提案する。本論文では、データセンターの水冷システムをより経済的なものにするために、温水冷却戦略とワークロードの変動に応じた適応冷却制御フレームワークを設計した。実際のハードウェアプロトタイプとGoogleとAlibabaのクラスタトレースに基づいて、ハイブリッド水冷設計を評価した。その結果、従来の水冷システムと比較して、58.72%〜78.43%のエネルギー消費量を削減し、冷却のミスマッチに対応することができた。</p>
</li>
<li>
<p>DeepAttest: an end-to-end attestation framework for deep neural networks</p>
</li>
<li>
<p>ディープニューラルネットワーク（DNN）のための新しいハードウェアアーキテクチャが実用化され、デバイス提供者のハードウェアレベルの知的財産（IP）として考えられています。しかし、これらのインテリジェントデバイスが悪用される可能性があり、そのような脆弱性は特定されていない。このようなインテリジェントなプラットフォームの無秩序な利用や、ハードウェアレベルの知的財産保護の欠如は、デバイス提供者の商業的優位性を損ない、信頼性の高い技術移転を妨げるものである。我々の目標は、様々なプラットフォーム上のDNNアプリケーションのためのハードウェアレベルのIP保護と利用制御を提供する体系的な方法論を設計することである。IPの問題に対処するために、我々は、デバイスにマッピングされたDNNプログラムの正当性を証明する初のオンデバイスDNN認証方法であるDeepAttestを提示します。DeepAttestは、ターゲットプラットフォーム上に配置されたDNNの重みにエンコードされたデバイス固有のフィンガープリントを設計することで動作します。埋め込まれたフィンガープリント（FP）は、後にTEE（Trusted Execution Environment）のサポートを受けて抽出されます。事前に定義されたFPの存在は、照会されたDNNが認証されているかどうかを判断するための認証基準として使用されます。当社の認証フレームワークは、認証されたDNNプログラムのみが一致するFPを生成し、ターゲットデバイス上での推論を許可することを保証します。DeepAttestは、製造されたハードウェアのアプリケーション利用を制限するための実用的なソリューションをデバイスプロバイダに提供し、不正なDNNや改ざんされたDNNが実行されるのを防ぎます。</p>
<p>我々は、アルゴリズム/ソフトウェア/ハードウェアの共同設計アプローチを採用し、待ち時間とエネルギー消費の観点からDeepAttestのオーバーヘッドを最適化しています。導入を容易にするために、既存のディープラーニングフレームワークやTEEにシームレスに統合できるDeepAttestの高レベルAPIを提供し、ハードウェアレベルのIP保護と利用制御を実現しています。広範な実験により、DeepAttestの忠実性、信頼性、セキュリティ、効率性が、さまざまなDNNベンチマークやTEEサポートプラットフォーム上で実証されています。</p>
</li>
<li>
<p>TPShare: a time-space sharing scheduling abstraction for shared cloud via vertical labels</p>
</li>
<li>
<p>MesosやYARNなどの現在の共有クラウドオペレーティングシステム（クラウドOS）は、「事実上」2水平2層のクラウドプラットフォームアーキテクチャに基づいており、クラウドアプリケーションフレームワーク（Apache Sparkなど）を基盤となるリソース管理インフラストラクチャから切り離しています。各層は通常、時間共有または空間共有に基づいて、独自のタスクまたはリソース割り当てスケジューラを持っています。そのため、異なるレイヤのスケジューラは、必然的に切り離されており、お互いを意識することができず、リソース(例えばCPU)の浪費につながる可能性が高い。また、共有リソースの性能干渉により、テールレイテンシが損なわれる可能性もある。</p>
<p>本論文では、水平層間の重要なミッシング・コネクションを確立するための第一歩を踏み出す。本論文では、時空間共有スケジューリングの抽象化であるTPShareを提案する。垂直ラベルは、2つのレイヤ間で必要な情報を伝達する双方向（上下）のメッセージキャリアであり、可能な限り小さく抑えられている。これにより、異なるレイヤのスケジューラは、ラベルメッセージに応じたアクションを取ることができ、リソースの無駄を削減し、テールレイテンシを改善することができます。さらに、異なるクラウドアプリケーションフレームワークをサポートするためにラベルを定義することができる。</p>
<p>本研究では、Mesosと2つの一般的なクラウドアプリケーションフレームワーク(Apache SparkとFlink)にラベルメカニズムを実装し、時空間共有スケジューリングの抽象化の有効性を検討した。TPShareのラベルメッセージは、1)オンデマンドでのきめ細かいリソース提供、2)負荷を考慮したリソースフィルタリング、3)グローバルビューでのリソース需要のスケーリングを可能にし、最終的にはパフォーマンスとテールレイテンシを向上させることで、異なるレイヤの独立した時間共有または空間共有スケジューリングによるリソースの無駄とパフォーマンスの干渉を削減することができる。</p>
<p>TPShareで管理された8ノードクラスタ上に13のSparkバッチプログラムと4のFlinkレイテンシに敏感なプログラムを共同配置し、高速化、CPUとメモリの使用率、テールレイテンシを評価した。その結果、TPShareはMesosと比較して、CPUとメモリ使用率がさらに低い状態でSparkプログラムを大幅に高速化していることがわかりました。リソース利用率が高くなると、TPShareのスループットはMesosよりも飛躍的に大きくなります。Flinkプログラムでは、TPShareは99番目のテールレイテンシを平均48％、最大120％改善しました。</p>
</li>
<li>
<p>SoftSKU: optimizing server architectures for microservice diversity @scale</p>
</li>
<li>
<p>倉庫規模のデータセンターにおけるマイクロサービスの多様性と複雑性は、ユーザーベースの拡大と製品ポートフォリオの進化をサポートするために、ここ数年で急激に増大しています。マイクロサービスの多様性が加速しているにもかかわらず、ハードウェアリソースの柔軟性を維持し、調達の規模の経済性を維持し、認定/テストのオーバーヘッドを抑制するために、基盤となるサーバハードウェアの多様性を制限することが強く求められています。そのため、限られたサーバ CPU アーキテクチャ（別名「SKU」）が多様なマイクロサービスに対して性能とエネルギー効率を提供できるようにする戦略が急務となっています。この目的のために、我々はまず、Facebook の計算に最適化されたデータセンターのフリート上で実行されている上位 7 つのマイクロサービスの包括的な特性評価を行います。</p>
<p>その結果、OSとI/Oの相互作用、キャッシュミス、メモリ帯域幅の使用率、命令構成、CPUのストール動作に大きな多様性があることがわかりました。マイクロサービスごとにCPU SKUをカスタマイズすることは有益かもしれませんが、それは法外なことです。その代わりに、我々は、特定のマイクロサービスのためのプラットフォームを調整するために、我々は粗粒（例えば、ブートタイム）構成ノブを利用して、"ソフトSKU "を主張しています。本番環境でのA/Bテストを用いてソフトSKU設計空間の検索を自動化するツールμSKUを開発し、追加のハードウェア要件を必要とせずに、統計的に有意なパフォーマンス向上（ストックサーバと本番サーバでそれぞれ最大7.2%、4.5%のパフォーマンス向上）が得られることを実証しています。</p>
</li>
</ul>
<h3 id="session-5a-quantum">Session 5A: Quantum</h3>
<ul>
<li>
<p>Full-stack, real-system quantum computer studies: architectural comparisons and design insights</p>
</li>
<li>
<p>近年，量子コンピューティング（QC）は，小型の動作するプロトタイプが利用できるところまで進歩しています．NISQ（Noisy Intermediate-Scale Quantum）コンピュータと呼ばれるこれらのプロトタイプは、大規模なベンチマークや量子誤り訂正（QEC）には小さすぎますが、小規模なベンチマークを実行するのに十分なリソースを持っています。しかし、QCはまだ特定の好ましいデバイス実装技術を決定しておらず、異なるNISQプロトタイプでは、物理的なアプローチが非常に異なるため、デバイスやマシンの特性が大きく変化します。</p>
<p>私たちの研究では、QCシステムのフルスタック、ベンチマーク駆動型のハードウエア・ソフトウエア解析を行います。私たちは、ゲートセットの選択、通信トポロジー、ベンチマーク性能に影響を与える要因、コンパイラの最適化に関する基本的な設計上の疑問に取り組むために、QCアーキテクチャの可能性、ソフトウェア可視化ゲート、ソフトウェアの最適化を評価しています。クロステクノロジおよびクロスプラットフォームの設計上の重要な疑問に答えるために、本研究では、現在のQCのフロントランナーである超伝導およびトラップドイオンクビットを含む、さまざまなクビットデバイス技術を対象とした初のトップからボトムまでのツールフローを構築しました。我々は、我々のツールフローであるTriQを使用して、IBM、Rigetti、メリーランド大学の3つの異なるグループの7つの実行中のQCプロトタイプで実システム測定を行いました。全体的に、コンパイラのマイクロアーキテクチャの詳細を活用することで、ベンダーのツールフローと比較して、IBMでは28倍（ジオメーン3倍）、Rigettiでは2.3倍（ジオメーン1.45倍）、UMDTIでは1.47倍（ジオメーン1.17倍）のプログラム成功率が向上することを実証しました。さらに、QCのハードウェア・ソフトウェア・インターフェースにおけるこれらの実システムの経験から、異なるQC技術や通信トポロジー、低ノイズのプラットフォームでもノイズを考慮したコンパイルの価値についての観察と提言を行っています。これは、これまでに行われたクロスプラットフォームのリアルシステム QC 研究の中で最大規模のものであり、その結果は、今後の QC デバイスとコンパイラの設計に役立つ可能性を秘めています。</p>
</li>
<li>
<p>Statistical assertions for validating patterns and finding bugs in quantum programs</p>
</li>
<li>
<p>量子コンピューティング実験への関心の高まりを受けて、プログラマは量子アルゴリズムをプログラムコードとして記述するための新しいツールを必要としています。 古典的なプログラムのデバッグと比較して、量子プログラムのデバッグは、プログラマが量子プログラムの内部状態を調べる能力が限られていること、観測があってもその状態を解釈することが困難であること、量子プログラムを構築する際に何をチェックすべきかのガイドラインがまだないことなどから困難です。 本研究では、古典的な観測結果の統計的検定に基づいた量子プログラムのアサーションを提案する。 これにより、プログラマは、量子プログラムの状態が、古典的な状態、重ね合わせ状態、もつれた状態のいずれかで期待される値と一致するかどうかを判断することができるようになります。 我々は、既存の量子プログラミング言語を拡張し、量子アサーションを指定できるようにしました。 これらのアサーションを用いて、ファクタリング、検索、化学の3つのベンチマーク量子プログラムをデバッグしています。 また、どのようなバグが発生する可能性があるかを共有し、アサーションを配置してバグを防ぐために量子プログラミングパターンを使用するための戦略を示します。</p>
</li>
<li>
<p>Asymptotic improvements to quantum circuits via qutrits</p>
</li>
<li>
<p>量子計算は伝統的に量子ビット（qubits）で表現されてきました。この研究では、代わりに3レベルの量子ビットを考えます。これまでのqutritsを用いた研究では、log2(3)の2値から3値への圧縮係数のために、一定の係数しか改善されないことが実証されていませんでした。我々は，アンシラを使わずに一般化トッフォリゲートの対数的な深さ（ランタイム）分解を達成するためにqutritsを使った新しい手法を提示します-これは最高のクビットのみの等価物の線形深さよりも大幅に改善されます。また、我々の回路構成は、クビットのみの等価分解と比較して、2ビットのゲート数を70倍向上させています。これにより、量子ニューロンやGrover探索などの重要なアルゴリズムの回路コストを削減することができます。我々は、qutritsのためのオープンソースの回路シミュレータを、qutritsを動作させるためのコストを考慮した現実的な近い将来のノイズモデルとともに開発している。これらのノイズモデルのシミュレーション結果は、量子ビットのみのベースラインでは30％以下であったのに対し、我々の回路構成では90％以上の平均信頼性（忠実度）を示した。これらの結果は，量子計算のスケーリングに向けてqutritsが有望な道筋を提供することを示唆しています。</p>
</li>
<li>
<p>A stochastic-computing based deep learning framework using adiabatic quantum-flux-parametron superconducting technology</p>
</li>
<li>
<p>最近、Adiabatic Quantum-Flux-Parametron（AQFP）超電導技術が開発され、超電導ロジック・ファミリーの中で最も高いエネルギー効率を実現し、最先端のCMOSと比較して潜在的に104--105ゲインを達成している。2016年には、8万3,000JJ規模のAQFPベースの回路の作製とテストに成功し、AQFPを用いた大規模システムの実装のスケーラビリティと可能性が実証された。その結果、Deep Neural Network（DNN）推論の高速化を重要な例として挙げ、高性能コンピューティングやディープスペース分野でのAQFPの利用が有望視されます。</p>
<p>AQFPは、超高エネルギー効率に加えて、各ロジックゲートがACクロック信号で接続されているため、パイプラインが深く、RAWハザードを回避することが困難であることと、1つのAQFPバッファで真の乱数生成(RNG)が可能であり、CMOSのRNGよりもはるかに効率的であることの2つの特徴を持っています。これら2つの特徴により、AQFPは値表現に時間に依存しないビット列を使用する確率的コンピューティング(SC)技術と特に互換性があり、ディープ・パイプラインの性質と互換性があることを指摘します。また、従来の研究では、SCのDNNへの適用が検討されており、SCの方が近似計算との親和性が高いことから、その適合性が示されている。</p>
<p>本研究は、AQFP技術を用いたSCベースのDNNアクセラレーションフレームワークを開発した最初の研究である。AQFP回路のディープ・パイプラインの性質は、AQFPのアキュムレータ/カウンタを設計することの難しさを意味し、SCベースのDNNの先行設計を適切ではないものにしている。我々は、CONV層とFC層の異なる特性を考慮して、この制限を克服する。(i)FC層の内積計算はCONV層よりも入力数が多く、(ii)FC層よりもCONV層の方が正確な活性化関数が重要である。これらの知見に基づき、(i) ビトニックソートネットワークとフィードバックループを用いた CONV 層での和算と活性化関数の正確な統合、(ii) FC 層でのマジョリティゲートの連鎖に基づく低複雑度のカテゴライズブロックを提案する。また、完全な設計のために、(i)AQFPにおける超効率的な確率的数生成器、(ii)AQFPにおける高精度サブサンプリング(プーリング)ブロック、(iii)AQFP回路の要求に応じた更なる性能向上と自動バッファ/スプリッタ挿入のためのマジョリティ合成を開発した。実験結果は、提案するAQFPを用いたSCベースのDNNは、MNISTデータセットにおいて96%の精度を維持しながら、CMOSベースの実装と比較して最大6.8×104倍のエネルギー効率を達成できることを示唆している。</p>
</li>
<li>
<p>A quantum computational compiler and design tool for technology-specific targets</p>
</li>
<li>
<p>量子コンピューティングは、かつては理論的な分野に過ぎませんでしたが、物理的な量子技術のサイズ、能力、信頼性の向上に伴い、急速に進歩しています。一般的な量子コンピュータや特定のアプリケーションに特化したデバイスの能力を十分に活用するためには、仕様を最適化し、特定のアーキテクチャ上での実現にマッピングするためのコンパイラやツールを開発する必要があります。本研究では、量子コンピュータにアルゴリズムを合成するための技術とプロトタイプツールについて説明し、その評価を行う。最近報告された手法のほとんどは、技術的に依存しない可逆的なカスケードを生成するものであり、実際の技術的に依存するセルライブラリやデバイスの事前設定された相互接続性による制約を考慮することなく、機能的に完全な演算子のセットで構成されている。対照的に、我々のプロトタイプツールは、アルゴリズムを、コンピュータアーキテクチャに存在するプリミティブと接続性制約のセットで構成される技術的に依存する仕様に合成します。このツールは、実際のアーキテクチャ制約に基づいて最適化を実行し、実際のハードウェアとアーキテクチャパラメータから導き出される最適化コスト関数を使用することで、技術に依存した高品質な合成結果を実現します。さらに、我々のツールのもう一つの重要な側面は、最初に指定されたアルゴリズムが、最適化された技術的にマップされた出力と機能的に等価であることを保証する内部形式的等価性チェックを組み込んでいることです。量子コンピュータのIBM Qファミリーを対象とした実験結果が提供されています。</p>
</li>
</ul>
<h3 id="session-5b-communication">Session 5B: Communication</h3>
<ul>
<li>
<p>IntelliNoC: a holistic design framework for energy-efficient and reliable on-chip communication for manycores</p>
</li>
<li>
<p>現在、マルチコア・アーキテクチャのオンチップ通信に使用されているネットワーク・オン・チップ（NoC）は、技術のスケールアップに伴い、高いネットワーク・レイテンシ、過剰な消費電力、低い信頼性などの問題に直面しています。これらの問題を同時に解決することは、設計空間が爆発的に拡大し、多くのトレードオフを処理することが複雑になるため困難であることがわかっている。この論文では、アーキテクチャの革新を導入し、強化学習を利用して設計の複雑さを管理し、性能、エネルギー効率、信頼性を総合的に最適化するインテリジェントNoC設計フレームワークであるIntelliNoCを提案します。IntelliNoCは、3つのNoCアーキテクチャ技術を統合しています。(1)エネルギー効率を向上させるマルチファンクション適応チャネル（MFAC）、(2)信頼性を向上させる適応エラー検出/訂正および再送信制御、(3)過熱や疲労を防ぐためにNoCコンポーネントの電源を動的にオフにするストレス緩和バイパス機能です。これらの技術によって引き起こされる複雑な動的相互作用を処理するために，消費電力と面積オーバーヘッドを削減しつつ，耐故障性と性能を向上させることを目的として，Q-learningを用いた動的制御ポリシーを学習しています．PARSECベンチマークを用いたシミュレーションの結果、提案するIntelliNoCデザインは、ベースラインのNoCアーキテクチャと比較して、エネルギー効率を67％、平均故障時間（MTTF）を77％向上させ、エンドツーエンドのパケットレイテンシを32％、面積要件を25％減少させることを示している。</p>
</li>
<li>
<p>HALO: accelerating flow classification for scalable packet processing in NFV</p>
</li>
<li>
<p>NFV（Network Function Virtualization）は、汎用サーバ上に様々なネットワークサービスを柔軟かつ機動的に展開できることから、クラウドプラットフォームの新たなスタンダードとなっています。しかし、ソフトウェアパケット処理においては、最適化されていない性能に悩まされているのが現状です。我々の仮想スイッチの特性調査では、最新のサーバでは分類ルールの大部分が最終レベルキャッシュ（LLC）にキャッシュされているにもかかわらず、フロー分類がNFVにおけるパケット処理のスループットを制限する主要なボトルネックとなっていることが示されている。</p>
<p>このボトルネックを克服するために、我々は、フロー分類を高速化するための効果的なニアキャッシュコンピューティングソリューションであるHaloを提案します。Halo は、ほぼすべてのインテル® マルチコア CPU に搭載されている NUCA (Non-Uniform Cache Access) と CHA (Caching and Home Agent) で構成されるキャッシュ・アーキテクチャーのハードウェア並列性を利用しています。これは、各CHAコンポーネントにアクセラレータを関連付けることで、LLC内のフロー分類を高速化して拡張します。Halo をより汎用的なものにするために、x86--64 命令セットを拡張し、提案されたニアキャッシュ・アクセラレーターを利用するための 3 つの単純なデータ・ルックアップ命令を追加しました。Haloの開発にはフルシステムシミュレータgem5を用いた。ネットワークサービスの様々な実世界のワークロードを用いた実験の結果、Haloは基本的なフロールール検索操作のスループットを3.3倍向上させ、代表的なフロー分類アルゴリズムであるタプル空間探索を23.4倍に拡張し、最先端のソフトウェアベースのソリューションと比較して、コロケーションされたネットワークサービスの性能に悪影響を与えないことを実証しました。また、Haloは、最速でありながら高価な3元コンテンツ・アドレス指定可能メモリ（TCAM）よりも最大48.2倍のエネルギー効率を実現し、消費電力と面積のオーバーヘッドを最小限に抑えています。</p>
</li>
<li>
<p>Scalable interconnects for reconfigurable spatial architectures</p>
</li>
<li>
<p>近年、柔軟でエネルギー効率の高い計算アクセラレータとして、粗視化再構成可能アーキテクチャ（CGRA）の採用が増加しています。多様なアプリケーションをサポートしながら空間アーキテクチャを使用してパフォーマンスを実現するには、柔軟で高帯域幅のインターコネクトが必要です。最近の CGRA は広いデータパスを持つベクトル単位をサポートしているため、動的性、通信の粒度、およびプログラマビリティのバランスをとるインターコネクトの設計は困難な課題です。</p>
<p>この作業では、空間アーキテクチャ・インターコネクトのダイナミズム、粒度、およびプログラマビリティの空間を探求します。まず、いくつかのベンチマークの通信パターンを特徴づけ、リンクの不均衡な帯域幅要件、ファンアウト、データ幅を示すことから始める。次に、アプリケーションをスタティックネットワークとダイナミックネットワークの両方にマッピングし、デッドロックの自由を保証するために仮想チャネル割り当てを実行するコンパイラスタックについて説明します。最後に、サイクル精度の高いシミュレータと28nmのASIC合成を使用して、さまざまなベンチマークのために、特定されたデザイン空間全体のパフォーマンス、面積、および消費電力の詳細な評価を実行します。最良のネットワーク設計は、アプリケーションと基礎となるアクセラレータ・アーキテクチャの両方に依存していることを示しています。ネットワーク性能はストリーミング・アクセラレータの帯域幅と強く相関し、生の帯域幅をスケーリングした方がスタティック・ネットワークの方が面積とエネルギー効率が高いことがわかりました。高帯域幅のスタティックネットワークからのフォールバックとしてダイナミックネットワークを使用することで、アプリケーションマッピングを最適化して、より少ないデータ量で移動できることを示しています。このスタティック-ダイナミックハイブリッドネットワークは、純粋なスタティックネットワークと純粋なダイナミックネットワークに比べて、それぞれ1.8倍のエネルギー効率と2.8倍のパフォーマンスの優位性を提供します。</p>
</li>
<li>
<p>CoNDA: efficient cache coherence support for near-data accelerators</p>
</li>
<li>
<p>特殊なオンチップアクセラレータは、コンピューティングシステムのエネルギー効率を向上させるために広く使用されています。最近のメモリ技術の進歩により、ニア・データ・アクセラレータ（NDA）が可能になりました。このNDAは、オフチップでメインメモリの近くに存在し、オンチップ・アクセラレータよりも大きなメリットをもたらします。しかし、すでにアクセラレータにとって大きな課題となっているシステムの残りの部分とのコヒーレンスの確保は、NDAではより困難になっています。これは、(1)NDAとCPU間の通信コストが高いこと、(2)NDAアプリケーションではオフチップでのデータ移動が多いことが理由です。その結果、本研究で示すように、既存のコヒーレンスメカニズムでは、NDAの利点のほとんどが失われている。その結果、既存のコヒーレンス機構では、NDAの利点のほとんどが排除されていることがわかった。</p>
<p>我々の観測に基づいて、我々は、NDAが必要なコヒーレンスパーミッションをすべて持っていると仮定して、NDAがNDAカーネルを楽観的に実行することを可能にするコヒーレンスメカニズム、CoNDAを提案する。この楽観的な実行により、CoNDAはNDAによって実行されたメモリアクセスとシステムの残りの部分のメモリアクセスに関する情報を収集することができます。CoNDAはこの情報を利用して不要なコヒーレンス要求の実行を回避し、コヒーレンスのためのデータ移動を大幅に削減します。</p>
<p>最新のグラフ処理とハイブリッドインメモリデータベースのワークロードを使用してCoNDAを評価しています。中程度のデータセットサイズで動作するすべてのワークロードで平均してみると、CoNDAは最高性能の先行コヒーレンスメカニズム（CPUのみ/NDAのみのシステムで66.0%/51.7%）と比較して19.6%性能を向上させ、最もエネルギー効率の高い先行コヒーレンスメカニズム（CPUのみのシステムで43.7%）と比較してメモリシステムの消費電力を18.0%削減しています。CoNDAは、コヒーレンスにコストをかけない理想的なメカニズムの性能とエネルギーの10.4%と4.4%の範囲内に収まっています。CoNDAの利点は、大規模なデータセットでは増加します。CoNDAは、最高性能の先行コヒーレンス・メカニズムと比較して38.3%の性能向上（CPUのみ/NDAのみの8.4倍/7.7倍）を実現し、理想的なノーコストのコヒーレンス・メカニズムの10.2%以内に収まっています。</p>
</li>
<li>
<p>Designing vertical processors in monolithic 3D</p>
</li>
<li>
<p>垂直方向に積層されたプロセッサは、ワイヤ遅延の低減、低エネルギー消費、および小さなフットプリントのメリットが得られます。このような設計は、ワイヤ長が短く、熱特性が良く、高集積化が可能な技術であるモノリシック3D（M3D）によって可能になります。現在のM3D技術では、製造上の制約により、スタック内の層が非対称になっており、一番下の層の方が比較的高い性能を持っています。</p>
<p>この論文では、M3D用のプロセッサをどのように分割するかを検討します。一番上の層には低性能のトランジスタがあることを考慮して、論理構造とストレージ構造を2つの層に分割しています。論理構造では、最下層にクリティカルパスを配置します。ストレージ構造では、ハードウェアを不均等に分割し、最上層にはより大きなアクセストランジスタを搭載したより少ないポートを割り当てたり、より大きなビットセルを搭載したより短いビットセルサブアレイを割り当てたりしています。M3D技術を保守的に仮定した場合、M3Dコアは2Dコアよりも平均25%高速にアプリケーションを実行し、消費電力は39%削減されることがわかりました。積極的にテクノロジーを仮定した場合、M3Dコアのパフォーマンスはさらに向上し、2Dコアよりも平均38%速く、消費電力は41%少なくなります。さらに、同じようなパワーバジェットでは、M3Dマルチコアは2Dマルチコアの2倍のコア数を使用し、平均92%高速で39%少ないエネルギーでアプリケーションを実行することができます。最後に、M3Dコアは熱効率に優れています。</p>
</li>
</ul>
<h3 id="session-6a-software-hardware-and-prediction">Session 6A: Software-Hardware and Prediction</h3>
<ul>
<li>
<p>Time squeezing for tiny devices</p>
</li>
<li>
<p>ダイナミック・タイミング・スラックは、超低消費電力の組み込みシステムの非効率性を解消するための有力な機会として浮上してきました。このスラックは、すべての信号がクロック信号よりもはるかに前にロジック・パスを伝搬した場合に発生します。これが適切に識別されると、システムはこの未使用のサイクルタイムを利用してエネルギーを節約することができます。この論文では、他の方法では不可能なタイミング・スラックのための新たな機会を開くコンパイラとアーキテクチャの共同設計について述べる。クロスレイヤ最適化により、ハードウェアとコンパイラに新しいメカニズムを導入し、実行中の時間を効果的に短縮することで、回路レベルのタイミング特定の利点を向上させます。このアプローチは、特に小型の組込み機器に適しています。完全なプロセッサのゲートレベルモデルを用いた評価では、開発者への透明性を維持しつつ、13のワークロードにおいて、我々の共同設計により、元のエネルギー消費量の40.5％（既存のクロックスケジューリング手法と比較して16.5％の追加）を節約できることが示されています。</p>
</li>
<li>
<p>XPC: architectural support for secure and efficient cross process call</p>
</li>
<li>
<p>マイクロカーネルは、セキュリティ、耐障害性、モジュール性、カスタマイズ性など、多くの魅力的な機能を備えており、最近では学界と産業界の両方で関心が高まっています（seL4、QNX、GoogleのFuchsia OSなど）。しかし、マイクロカーネルのアキレス腱として知られているIPC(プロセス間通信)は、いまだにOSの全体的な(貧弱な)パフォーマンスの主な要因となっています。さらに、IPC は Android Linux のようなモノリシックカーネルにおいても重要な役割を果たしています。タグ付きメモリやケイパビリティのようなハードウェアソリューションは、分離のためにページテーブルを置き換えますが、通常、新しいハードウェアプリミティブを適応させるために、既存のソフトウェアスタックへの非自明な変更を必要とします。本論文では、高速で安全な同期IPCのためのハードウェア支援OSプリミティブであるXPC(Cross Process Call)を提案する。XPC は、カーネルにトラップすることなく IPC 呼び出し元と呼び出し元を直接切り替えることを可能にし、 呼び出しチェーンを介した複数プロセス間のメッセージの受け渡しをコピーすることなくサポートする。このプリミティブは、従来のアドレス空間ベースの分離機構と互換性があり、既存のマイクロカーネルやモノリシックカーネルに簡単に統合することができます。FPGAボードを搭載したRocket RISC-VコアをベースにXPCのプロトタイプを実装し、2つのマイクロカーネル実装であるseL4とZircon、モノリシックカーネル実装であるAndroid Binderを評価用に移植しました。また、GEM5シミュレータ上でXPCを実装し、その汎用性を検証した。その結果、XPCはIPCコールのレイテンシを664から21サイクルに削減し、Android Binderで最大54.2倍の性能向上を実現し、マイクロカーネル上の実アプリケーションの性能をSqlite3で1.6倍、HTTPサーバで10倍に向上させることができ、最小限のハードウェアリソースコストで実現できることがわかりました。</p>
</li>
<li>
<p>AxMemo: hardware-compiler co-design for approximate code memoization</p>
</li>
<li>
<p>歴史的に、汎用プロセッサの継続的な改良は、IT 産業の経済的な成功と成長を後押ししてきました。しかし、トランジスタのスケーリングや従来の最適化技術の効果が薄れてきているため、従来の常識を覆す必要が出てきています。近似計算は、このような従来の技術にとらわれない手法の一つであり、汎用処理の限界を押し広げることが期待されている。本論文では、サイバーフィジカル領域で一般的に使用され、Internet of Thingsの構成要素となる可能性のあるプロセッサに近似演算を採用することを目的としている。そのために、コードブロックの入力におけるデータの類似性に起因する計算の冗長性を利用するために、AxMemoを提案する。このような入力動作は、サイバーフィジカルシステムでは、実世界のデータを扱うため、当然ながら冗長性を持つことが一般的である。したがって、コストのかかる浮動小数点演算を限られた入力数で置き換える既存のメモ化技術とは対照的に、AxMemoは、潜在的に多くの入力を持つコードブロックのメモ化に焦点を当てています。そのため、AxMemoは、長い命令列を少数のハッシュとルックアップ操作で置き換えることを目的としています。動的命令の数を減らすことで、AxMemoは、プロセッサのパイプラインに命令を通すことによるフォンノイマンと実行オーバーヘッドを完全に軽減します。AxMemoが直面している課題は、マルチ入力の組み合わせごとにユニークなシグネチャを生成できる低コストのハッシュ機構を提供することです。この課題に対処するために、我々は入力をハッシュ化するためにCRC(Cyclic Redundancy Checking)の新しい使用法を開発しました。ルックアップテーブルのヒット率を向上させるために、AxMemoは2レベルのメモライズルックアップを採用しており、これは小型の専用SRAMと最終レベルキャッシュの予備ストレージを利用している。これらのソリューションにより、AxMemoは、同じ基礎となるハードウェアを使用して、入力サイズとタイプが変化する比較的大きなコード領域を効率的にメモすることができます。私たちの実験では、AxMemoは10のベンチマークで平均0.2%の品質損失で2.64倍のスピードアップと2.58倍のエネルギー削減を実現していることが示されています。これらの利点は、わずか2.1%のエリアオーバーヘッドで実現しています。</p>
</li>
<li>
<p>Translation ranger: operating system support for contiguity-aware TLBs</p>
</li>
<li>
<p>仮想メモリ（VM）はプログラミングの手間を軽減しますが、高いアドレス変換オーバーヘッドに悩まされることがあります。アーキテクトは従来、TLB（Translation Lookaside Buffer）の容量を増やすことで対処してきましたが、この方法ではかなりのハードウェアリソースを必要とします。1 つの有望な代替案は、TLB 内のページ翻訳エンコーディングを圧縮するために、ソフトウェアで生成された翻訳コンティギュイティに依存することです。これを可能にするには、オペレーティングシステム（OS）は、物理フレームの空間的に隣接したグループを仮想ページの連続したグループに割り当てる必要があり、そうすることで、これらの連続した翻訳をハードウェアで圧縮または合体することができます。残念ながら、現代のOSは現在のところ、多くの実世界のシナリオで翻訳の連続性を保証していません; システムが長時間オンラインのままであるため、そのメモリは断片化されてしまう可能性がありますし、断片化されてしまいます。</p>
<p>我々は、以前の連続性生成の提案がメモリの断片化と格闘している場合でも、失われた翻訳連続性を回復するOSサービスであるTranslation Rangerを提案します。Translation Rangerは、散在した物理フレームを積極的に連続領域に合体させることで連続性を向上させ、アプリケーションの変更を必要とせずに、任意の連続性を意識したTLBで活用することができる。我々は実際のハードウェア上でLinuxにTranslation Rangerを実装して評価し、Linuxのデフォルト構成より40倍大きい連続メモリ領域を生成し、通常は128以上の連続翻訳領域を持たない120GBメモリのTLBカバレッジを可能にすることを発見しました。これは、2%未満のランタイムオーバーヘッドで達成されており、Translation Rangerが提供するTLBカバレッジの改善によって、この数値を上回っています。</p>
</li>
<li>
<p>Bouncer: static program analysis in hardware</p>
</li>
<li>
<p>組込みシステムの安全性とセキュリティを議論するとき、私たちは一般的にソフトウェアチェック（静的または動的）とハードウェアチェック（動的）に分けて考えています。他の人が指摘しているように、ハードウェアチェックは効率性だけではありません。また、他のソフトウェアが正しく機能しているかどうかに依存する必要はほとんどなく、システムの動作に直接配線されているため、迂回することは難しいか、不可能です。我々は、メモリエラー、無効な制御フロー、および他のいくつかの望ましくない特性を持つすべてのプログラムバイナリがデバイスにロードされるのを防ぐ静的解析のために特別な目的のハードウェアを使用する実験的な新しい組み込みシステムを探究しています。スタティック解析では、多くの場合、命令レベルではなくバイナリレベルでの検査が必要となります。我々は、利用可能なスクラッチパッドメモリを使用して慎重に構築されたハードウェアステートマシンが、組み込みプログラムストアにロードされた機能的なバイナリを、ストリーミング的かつ検証可能な非バイパス的な方法で直接ハードウェア内で効率的にチェックできることを示しています。このシステムは驚くほど小さく（0.0079 mm2以下）、効率的（1命令あたり約60サイクルの平均スループットでバイナリをチェックできます）でありながら、セキュリティや安全性の懸念の原因となる多くの脆弱な動作から解放された実行が保証されています。これは、静的解析がハードウェアレベルで実装された初めての事例であり、より複雑なハードウェアチェックのプロパティへの扉を開くものと考えています。</p>
</li>
<li>
<p>Efficient invisible speculative execution through selective delay and value prediction</p>
</li>
<li>
<p>最近の高性能汎用 CPU のベースとなっている投機的実行は、多くのセキュリティ攻撃を可能にすることが最近明らかになっています。これらの攻撃はすべて、共通の動作を中心としています。投機的な実行の間、システムのアーキテクチャ状態は、投機が検証されるまで変更されません。誤判定が発生した場合、アーキテクチャの状態に影響を与える可能性のあるものはすべて元に戻され（潰され）、正しく再実行されます。しかし、マイクロアーキテクチャーの状態については、同じことは言えません。通常はユーザには見えないが、マイクロアーキテクチャ状態への変化は様々なサイドチャネルを通じて観察することができ、メモリ階層によるタイミングの違いは最も一般的で悪用しやすいものの1つである。推測されたサイドチャネルを利用して、ソフトウェアやハードウェアのチェックを回避して情報を漏洩させる攻撃を実行することができます。これらの攻撃のうち、最も悪名高いのは Spectre と Meltdown ではないでしょうか。</p>
<p>この作品では、メモリ階層における投機的な実行によって引き起こされるマイクロアーキテクチャの状態変化を減らすための独自の解決策を提示します。これは、L1 データキャッシュにヒットするアクセスのみを許可すれば、投機が検証されるまでマイクロアーキテクチャの変化を簡単に隠すことができるという観察に基づいています。同時に、L1で失敗する負荷を値予測することでストールを防ぐことを提案します。値の予測は、投機的ではあるが、コアの外には見えない、目に見えない投機の一形態を構成している。我々のソリューションを評価し、メモリ階層における目に見えるマイクロアーキテクチャーの変化を防ぎながら、性能とエネルギーコストをそれぞれ11%と7%に抑えることができることを示します。これと比較して、現在の最新のソリューションであるInvisiSpecでは、性能が46%低下し、エネルギーが51%増加しています。</p>
</li>
</ul>
<h3 id="session-6b-acceleration-and-non-traditional-architecture">Session 6B: Acceleration and Non-Traditional Architecture</h3>
<ul>
<li>
<p>Stream-based memory access specialization for general purpose processors</p>
</li>
<li>
<p>技術のスケーリングには深刻な制限があるため、アーキテクトは計算プリミティブ(ベクトル命令、ループアクセラレータなど)のための汎用プロセッサに特化した技術革新を行ってきました。一般的な原則は、豊富なセマンティクスをISAに公開することです。メモリアクセスパターンのより豊かなセマンティクスが、メモリと通信の効率を向上させるためにも使用できるかどうかを探る機会となります。2つの重要な未解決の問題は、どのようにしてより高いレベルのメモリ情報を伝えるか、そしてどのようにしてハードウェアでこの情報を活用するかということです。</p>
<p>我々は、メモリアクセスの大部分が少数の単純なパターンに従っていることを発見しました；我々はこれらをストリームと呼んでいます（例えば、アフィン、間接的）。ストリームはコア実行から切り離されることが多く、パターンは有用な動作を表現するのに十分な長さまで持続します。したがって、我々のアプローチは、ストリームをISAプリミティブとして表現することであり、これにより、メモリレイテンシを隠すためのストリームアクセスをプリフェッチし、アドレス計算を削除してメモリインターフェースを最適化するためのセミバインディングデカップリングアクセスを行い、最終的にはキャッシュポリシーを通知することが可能になると主張しています。</p>
<p>この作品では、FIFOベースのインターフェイスを使用してコアと対話するデカップリングストリームのためのISA拡張を提案します。我々は、アグレッシブなワイドイシューOOOコア上で前述の機会のそれぞれに対する最適化を実装し、SPEC CPU 2017およびCortexSuite[1, 2]を用いて評価する。すべてのワークロードにわたって、ハードウェア・ストライド・プリフェッチと比較して約1.37倍の高速化とエネルギー効率の向上を観測しています。</p>
</li>
<li>
<p>Using SMT to accelerate nested virtualization</p>
</li>
<li>
<p>IaaSデータセンターは仮想マシン（VM）をクライアントに提供しており、クライアントは独自の仮想化環境をデプロイし、VMの中でVMを実行することがあります。これは入れ子仮想化として知られています。</p>
<p>VM は、仮想 I/O デバイスの操作などのタスクを実行するためにハイパーバイザーにトラップすることが多いため、ベアメタル実行よりも本質的に遅くなります。各 VM トラップは、VM とハイパーバイザーのコンテキストを切り替えるために数十個のレジスタをロードして格納する必要があり、その結果、コストのかかるランタイムオーバーヘッドが発生します。従来の仮想化環境では、すべての VM トラップが少なくとも 2 倍のトラップをトリガするため、ネストされた仮想化はこれらのオーバーヘッドをさらに拡大します。</p>
<p>我々は、ネスト仮想化における VM トラップのオーバーヘッドを軽減するために、同時マルチスレッド（SMT）コアのレプリケートされたスレッド実行リソースを活用することを提案します。提案するアーキテクチャは、異なる VM とハイパーバイザをコアの別個のハードウェアスレッド上にコロケーションするシンプルなメカニズムを導入し、VM トラップのコストのかかるコンテキストスイッチをシンプルなスレッドストールとレジュームイベントに置き換えます。より具体的には、SMTコアの各スレッドは独自のレジスタセットを持っているため、VMとハイパーバイザ間のトラップは、コストのかかるコンテキストスイッチを必要とせず、単にコアが別のハードウェアスレッドから命令をフェッチすることを必要とするだけである。さらに、私たちのスレッド間通信メカニズムでは、ハイパーバイザーがその下位のVMのレジスタに直接アクセスして操作することができます。</p>
<p>我々のアーキテクチャのモデルでは、I/Oレイテンシが2.3倍、帯域幅が2.6倍向上しています。また、既存のSMTアーキテクチャを使用したシステムのソフトウェアのみのプロトタイプを示し、I/Oレイテンシと帯域幅がそれぞれ最大1.3倍と1.5倍向上し、さまざまな実世界のアプリケーションで1.2--2.2倍の高速化を実現しました。</p>
</li>
<li>
<p>Master of none acceleration: a comparison of accelerator architectures for analytical query processing</p>
</li>
<li>
<p>ハードウェアアクセラレータは、デナードスケーリングの終焉とムーアの法則の減速に対処するための有望なソリューションの一つです。規則的でバイトあたりの計算量が多い成熟したワークロードでは、アプリケーションを 1 つ以上のハードウェアモジュールにハード化するのが標準的なアプローチです。しかし、いくつかのアプリケーションでは、プログラマブルな同種アーキテクチャが望ましいことがわかります。</p>
<p>本論文では、以前に提案された分析的クエリ処理のためのヘテロジニアスなハードウェアアクセラレータと、ホモジニアスなシストリックアレイの代替品を比較した。その結果、大規模な設計ではヘテロジニアスとホモジニアスのアクセラレータは同等であり、小規模な設計ではホモジニアスの方が優れていることがわかりました。我々の分析では、この直観的ではない結果を説明し、ホモジニアスアーキテクチャの方が平均的なリソース利用率が高く、通信インフラストラクチャの相対的なコストが低いことを発見した。</p>
</li>
<li>
<p>Cryogenic computer architecture modeling with memory-side case studies</p>
</li>
<li>
<p>現代のコンピュータアーキテクチャは、主に電力の壁とメモリの壁のために、アーキテクチャの革新性の欠如に悩まされています。つまり、アーキテクチャの革新は、消費電力を法外に増加させ、その性能への影響は最終的に遅いメモリアクセスによって制限されてしまうため、実現不可能になってしまうのです。このような課題に対処するために、超低温で動作するコンピュータシステム（極低温コンピュータシステム）は、超低温では消費電力と電線抵抗率の両方が大幅に低減することが予想されるため、非常に有望な解決策として浮上してきました。しかし、極低温コンピュータは、既存のコンピュータシステムの動作やその費用対効果を十分に理解していないため、実現には至っていないのが現状です。</p>
<p>本論文では、まず、極低温メモリデバイスを組み込んだコンピュータアーキテクチャのシミュレーションツールであるCryoRAMを開発する。本研究では、最新のCMOSデバイスが信頼性を持って動作する77K温度（低コストの液体窒素を使用することで容易に達成できる温度）に焦点を当てています。また、完全な極低温コンピュータを構築する前のパイロット研究として、メモリデバイスの温度を下げることにも焦点を当てています。次に、モデル化ツールを用いて、温度を考慮したメモリデバイスとアーキテクチャ設計を提案し、DRAMアクセス速度を3.8倍、消費電力を9.2％削減することを目指します。最後に、極低温メモリを用いて、(1)サーバの性能を最大2.5倍に、(2)サーバの消費電力を平均6%削減し、(3)データセンターの電力コストを8.4%削減した3つの有望なケーススタディを紹介します。</p>
<p>一部の実験は業界機密の環境下で行われましたが、オープンソースのシミュレータのみを組み合わせた上に意図的に実装したモデリングツールとシミュレーションツールを公開します。</p>
</li>
<li>
<p>Cambricon-F: machine learning computers with fractal von neumann architecture</p>
</li>
<li>
<p>機械学習技術は、新興の商用アプリケーションに広く普及しているツールであり、さまざまな規模の多くの専用機械学習コンピュータが、組み込み機器、サーバ、データセンターに配備されています。現在、ほとんどの機械学習コンピュータのアーキテクチャは、いまだにプログラミングの生産性ではなく、パフォーマンスとエネルギー効率の最適化に焦点を当てている。しかし、シリコン技術の急速な発展に伴い、機械学習コンピュータの応用を妨げる性能や電力効率ではなく、プログラミングそのものやソフトウェアスタック開発を含むプログラミング生産性が重要な理由となってきている。</p>
<p>本論文では、同種のISAを用いて、均質で逐次的な多層・層相似型の機械学習計算機をシリーズ化したCambricon-Fを提案する。Cambricon-Fマシンは、その構成要素を反復的に管理するためのフラクタル・フォン・ノイマン・アーキテクチャを持っています：それはフォン・ノイマン・アーキテクチャを持ち、その処理構成要素（サブノード）は、フォン・ノイマン・アーキテクチャと同じISAを持つCambricon-Fマシンのままです。スケールの異なる異なるCambricon-Fインスタンスが共通のISA上で同じソフトウェアスタックを共有できるので、Cambricon-Fはプログラミングの生産性を大幅に向上させることができます。さらに、Cambricon-Fの高効率化を可能にするCambricon-Fアーキテクチャ設計における4つの主要な課題に取り組んでいる。我々は、異なるスケールのCambricon-Fインスタンス、すなわちCambricon-F100とCambricon-F1を実装している。GPUベースのマシン（DGX-1と1080Ti）と比較して、Cambricon-Fインスタンスは、平均で2.82倍、5.14倍の性能向上、8.37倍、11.39倍の効率向上を達成し、面積コストはそれぞれ74.5％、93.8％削減されています。</p>
</li>
<li>
<p>FloatPIM: in-memory acceleration of deep neural network training with high precision</p>
</li>
<li>
<p>Processing In-Memory (PIM)は、畳み込みニューラルネットワーク(CNN)の推論タスクを高速化する大きな可能性を示している。しかし、既存のＰＩＭアーキテクチャは、正確なＣＮＮモデルの訓練に不可欠な浮動小数点精度などの高精度計算をサポートしていない。さらに，既存のPIMアプローチのほとんどは，アナログ/ミックスドシグナル回路を必要とし，信頼性の低いマルチビット不揮発性メモリ(NVM)を利用したスケーリングができない．この論文では、トレーニングとテストの両方の段階でCNNを高速化する、完全にデジタル化されたスケーラブルなPIMアーキテクチャであるFloatPIMを提案する。FloatPIMはネイティブに浮動小数点表現をサポートしており、正確なCNN学習を可能にする。また、FloatPIMは隣接するメモリブロック間の高速な通信を可能にし、PIMアーキテクチャの内部データ移動を削減する。一般的な大規模ニューラルネットワークを用いて、ImageNetデータセット上でのFloatPIMの効率性を評価した。その結果、浮動小数点精度をサポートするFloatPIMは、固定小数点精度に制限のある既存のPIMアーキテクチャと比較して、最大5.1%高い分類精度を達成できることが示された。FloatPIMのトレーニングは、GTX 1080 GPU（PipeLayer [1] PIMアクセラレータ）と比較して、平均303.2×、48.6×（4.3×、15.8×）の高速化とエネルギー効率の向上を実現しています。テスト用には、FloatPIMもGPU(ISAAC [2] PIMアクセラレータ)と比較して、それぞれ324.8×と297.9×(6.3×と21.6×)の高速化とエネルギー効率を実現しています。</p>
</li>
</ul>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../gpgpu/" title="GPGPU 記事一覧" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                GPGPU 記事一覧
              </span>
            </div>
          </a>
        
        
          <a href="../llvm/" title="LLVM" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                LLVM
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.d9aa80ab.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
  </body>
</html>